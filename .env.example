# OpenAI (requires API key)
OPENAI_API_KEY="sk-..."
OPENAI_RESPONSES_MODEL_ID="gpt-4o-mini"

# LiteLLM Configuration (optional - for universal LLM proxy support)
# When USE_LITELLM=true, all agents will use LiteLLM client instead of OpenAI client
# This enables support for multiple LLM providers (OpenAI, Anthropic, Azure, etc.)
# with built-in caching and tracing capabilities
USE_LITELLM=false
# Model identifier - can be from any supported provider:
# - OpenAI: "gpt-4o-mini", "gpt-4o", "gpt-5-mini"
# - Anthropic: "anthropic/claude-3-5-sonnet-20241022", "anthropic/claude-3-5-haiku-20241022"
# - Azure: "azure/gpt-4o", "azure/gpt-5-mini"
# - See https://docs.litellm.ai/docs/providers for full list
LITELLM_MODEL="gpt-4o-mini"
# Optional: API key (can also use provider-specific env vars like OPENAI_API_KEY, ANTHROPIC_API_KEY)
LITELLM_API_KEY=""
# Optional: Base URL for custom endpoints (e.g., LiteLLM proxy server)
LITELLM_BASE_URL=""
# Optional: Request timeout in seconds (default: 600)
LITELLM_TIMEOUT=600

# Azure OpenAI (requires endpoint and key or credential)
AZURE_OPENAI_ENDPOINT="https://your-resource.openai.azure.com/openai/v1"
AZURE_OPENAI_RESPONSES_DEPLOYMENT_NAME="gpt-5-mini"
AZURE_OPENAI_API_VERSION="preview"
# Optional: Use AZURE_OPENAI_API_KEY or Azure CLI credentials

# Azure AI Foundry (optional - required for Mem0 context provider)
AZURE_AI_PROJECT_ENDPOINT="https://your-project.api.azureml.ms"
AZURE_AI_MODEL_DEPLOYMENT_NAME="gpt-5-mini"
# Authentication: Use Azure CLI (`az login`) or managed identity


# Azure AI Search Configuration
AZURE_AI_SEARCH_ENDPOINT=your-azure-ai-search-endpoint
AZURE_AI_SEARCH_KEY=your-azure-ai-search-key

# Azure OpenAI Deployed Model Names
AZURE_OPENAI_CHAT_COMPLETION_DEPLOYED_MODEL_NAME=your-chat-completion-model-name
AZURE_OPENAI_EMBEDDING_DEPLOYED_MODEL_NAME=your-embedding-model-name

# Optional: For future phases
DATABASE_URL=postgresql://user:pass@localhost:5432/agenticfleet
MEM0_API_KEY=your_mem0_api_key
NEON_DATABASE_URL=postgresql://user:pass@ep-neon-host.neon.tech/dbname
REDIS_URL=redis://default:your_redis_password@your_redis_host:port

# Observability (OpenTelemetry)
# Set ENABLE_OTEL=true when you have an OpenTelemetry collector running
ENABLE_OTEL=false
ENABLE_SENSITIVE_DATA=false
OTLP_ENDPOINT=http://localhost:4317
# Optional: Azure Application Insights
# APPLICATIONINSIGHTS_CONNECTION_STRING="InstrumentationKey=..."
