{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50ed36a0",
   "metadata": {},
   "source": [
    "# AgenticFleet Fleet Support GEPA Notebook\n",
    "\n",
    "This notebook adapts the [DSPy GEPA Facility Support Analyzer tutorial](https://dspy.ai/tutorials/gepa_facilitysupportanalyzer/) to the AgenticFleet codebase. We will walk through defining a fleet-support-specific signature, building a custom DSPy module, preparing training data, compiling with `BootstrapFewShot`, and evaluating the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376877c1",
   "metadata": {},
   "source": [
    "## 1. Import Dependencies and Configure DSPy\n",
    "\n",
    "We load project utilities, configure Python paths, and initialize DSPy's LM backend with the same model referenced in `config/workflow_config.yaml` (`gpt-4.1`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb4d3b2e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'agentic_fleet.config.loader'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     14\u001b[39m     sys.path.append(\u001b[38;5;28mstr\u001b[39m(SRC_PATH))\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdspy\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01magentic_fleet\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mloader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_config\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01magentic_fleet\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdspy_modules\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mreasoner\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DSPyReasoner\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01magentic_fleet\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdspy_modules\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mworkflow_signatures\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EnhancedTaskRouting\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'agentic_fleet.config.loader'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "import inspect\n",
    "from pathlib import Path\n",
    "from types import SimpleNamespace\n",
    "from typing import List\n",
    "\n",
    "# Ensure repo src/ is on the path for absolute imports\n",
    "PROJECT_ROOT = Path(\".\").resolve()\n",
    "SRC_PATH = PROJECT_ROOT / \"src\"\n",
    "if str(SRC_PATH) not in sys.path:\n",
    "    sys.path.append(str(SRC_PATH))\n",
    "\n",
    "import dspy\n",
    "from agentic_fleet.config.loader import load_config\n",
    "from agentic_fleet.dspy_modules.reasoner import DSPyReasoner\n",
    "from agentic_fleet.dspy_modules.workflow_signatures import EnhancedTaskRouting\n",
    "from agentic_fleet.utils.gepa_optimizer import (\n",
    "    load_example_dicts,\n",
    "    prepare_gepa_datasets,\n",
    "    build_routing_feedback_metric,\n",
    ")\n",
    "\n",
    "# Configure DSPy using the model defined in workflow config (defaults to gpt-4.1)\n",
    "workflow_config = load_config()\n",
    "dspy_model = workflow_config.get(\"dspy\", {}).get(\"model\", \"gpt-4.1\")\n",
    "\n",
    "lm = dspy.LM(model=dspy_model, max_tokens=1024)\n",
    "dspy.configure(lm=lm)\n",
    "\n",
    "print(f\"Configured DSPy with model: {dspy_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877c9e27",
   "metadata": {},
   "source": [
    "## 2. Define the Fleet Support Signature\n",
    "\n",
    "AgenticFleet already ships with an enhanced routing signature (`EnhancedTaskRouting`) in `src/agentic_fleet/dspy_modules/workflow_signatures.py`. We inspect it directly so the notebook stays aligned with the production supervisor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f78a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"EnhancedTaskRouting signature (source excerpt):\\n\")\n",
    "print(inspect.getsource(EnhancedTaskRouting))\n",
    "\n",
    "print(\"\\nInput fields:\")\n",
    "for name, field in EnhancedTaskRouting.input_fields().items():\n",
    "    print(f\"- {name}: {field.desc}\")\n",
    "\n",
    "print(\"\\nOutput fields:\")\n",
    "for name, field in EnhancedTaskRouting.output_fields().items():\n",
    "    print(f\"- {name}: {field.desc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afa9cc8",
   "metadata": {},
   "source": [
    "## 3. Create the Agentic Fleet Module\n",
    "\n",
    "We reuse the production `DSPyReasoner` module (the same class that powers the Supervisor workflow). It wires multiple DSPy submodules—task analysis, routing, progress, and tool planning—under one interface that GEPA can optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a0386a",
   "metadata": {},
   "outputs": [],
   "source": [
    "reasoner = DSPyReasoner(use_enhanced_signatures=True)\n",
    "print(\"Named predictors exposed to GEPA:\")\n",
    "print([name for name, _ in reasoner.named_predictors()])\n",
    "\n",
    "# Quick smoke test using a lightweight task routed through the router signature\n",
    "sample_prediction = reasoner(\n",
    "    task=\"Summarize the latest GEPA optimizations for the exec weekly report\",\n",
    "    team_capabilities=(\n",
    "        \"Planner: decomposes projects.\\n\"\n",
    "        \"Researcher: runs Tavily searches.\\n\"\n",
    "        \"Writer: drafts polished updates.\"\n",
    "    ),\n",
    "    available_tools=\"TavilySearchTool, HostedCodeInterpreterTool\",\n",
    "    current_context=\"Need executive-friendly tone, highlight latency gains.\",\n",
    ")\n",
    "\n",
    "print(\"\\nSample routing decision:\")\n",
    "print(\n",
    "    {\n",
    "        \"assigned_to\": getattr(sample_prediction, \"assigned_to\", []),\n",
    "        \"execution_mode\": getattr(sample_prediction, \"execution_mode\", \"delegated\"),\n",
    "        \"tool_plan\": getattr(sample_prediction, \"tool_plan\", []),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12995b3",
   "metadata": {},
   "source": [
    "## 4. Load and Prepare Fleet Support Data\n",
    "\n",
    "Supervisor training data now lives exclusively in `src/agentic_fleet/data/supervisor_examples.json`. Use `scripts/merge_supervisor_examples.py` if you need to ingest additional examples (it will merge any extra files and regenerate the canonical dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c48011b",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_path = PROJECT_ROOT / \"src\" / \"agentic_fleet\" / \"data\" / \"supervisor_examples.json\"\n",
    "records = load_example_dicts(str(examples_path))\n",
    "\n",
    "if not records:\n",
    "    raise RuntimeError(\n",
    "        \"No training data found. Run scripts/merge_supervisor_examples.py to populate the dataset.\"\n",
    "    )\n",
    "\n",
    "train_examples, val_examples = prepare_gepa_datasets(\n",
    "    base_examples_path=str(examples_path),\n",
    "    base_records=records,\n",
    "    val_split=0.2,\n",
    "    seed=13,\n",
    ")\n",
    "\n",
    "print(f\"Training examples: {len(train_examples)} | Validation examples: {len(val_examples)}\")\n",
    "\n",
    "print(\"\\nExample record:\")\n",
    "sample = train_examples[0]\n",
    "print({\n",
    "    \"task\": sample.task,\n",
    "    \"assigned_to\": sample.assigned_to,\n",
    "    \"execution_mode\": sample.execution_mode,\n",
    "    \"tool_requirements\": sample.tool_requirements,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0898d1",
   "metadata": {},
   "source": [
    "## 5. Define Evaluation Metrics\n",
    "\n",
    "Our metric encourages the agent to produce correct diagnoses and actionable plans. We score partial credit for overlapping agent/tool selections so GEPA can receive granular feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2790bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "routing_feedback_metric = build_routing_feedback_metric()\n",
    "print(\"Metric ready – returns GEPA-friendly score + feedback text.\")\n",
    "\n",
    "# Demonstrate feedback using a real training example and a deliberately bad prediction\n",
    "gold = train_examples[0]\n",
    "bad_prediction = SimpleNamespace(\n",
    "    task=gold.task,\n",
    "    assigned_to=[\"Writer\"],  # intentionally wrong agent\n",
    "    execution_mode=\"delegated\",\n",
    "    tool_requirements=[],\n",
    "    latency_budget=\"low\",\n",
    ")\n",
    "\n",
    "score_feedback = routing_feedback_metric(gold, bad_prediction)\n",
    "print(f\"Score: {score_feedback.score:.2f}\\n\")\n",
    "print(\"Feedback excerpt:\\n\")\n",
    "print(\"\\n\".join(score_feedback.feedback.splitlines()[:8]))\n",
    "\n",
    "\n",
    "def bootstrap_routing_metric(example, prediction, trace=None):\n",
    "    \"\"\"Lightweight scorer used by BootstrapFewShot (mirrors compiler.py).\"\"\"\n",
    "    gold_agents = {agent.strip().lower() for agent in str(example.assigned_to).split(\",\") if agent}\n",
    "    pred_agents = {agent.strip().lower() for agent in getattr(prediction, \"assigned_to\", [])}\n",
    "    assignment_score = 1.0 if gold_agents & pred_agents else 0.0\n",
    "    mode_score = 1.0 if getattr(prediction, \"execution_mode\", \"\") == example.execution_mode else 0.0\n",
    "    return (assignment_score * 0.7) + (mode_score * 0.3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abab793",
   "metadata": {},
   "source": [
    "## 6. Compile the Agent with BootstrapFewShot\n",
    "\n",
    "We warm-start the fleet module with DSPy's `BootstrapFewShot` teleprompter so the LM observes a few good demonstrations before GEPA fine-tuning. This mirrors the standard AgenticFleet optimization pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29ae4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.teleprompt import BootstrapFewShot\n",
    "\n",
    "max_demos = min(6, len(train_examples)) or 1\n",
    "teleprompter = BootstrapFewShot(\n",
    "    metric=bootstrap_routing_metric,\n",
    "    max_bootstrapped_demos=max_demos,\n",
    "    max_labeled_demos=max_demos,\n",
    ")\n",
    "\n",
    "print(\"Running BootstrapFewShot compilation against supervisor dataset...\")\n",
    "compiled_reasoner = teleprompter.compile(reasoner, trainset=train_examples)\n",
    "print(\"Compilation complete. You can now hand this module to GEPA for further tuning.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a3066f",
   "metadata": {},
   "source": [
    "## 7. Evaluate and Inspect Results\n",
    "\n",
    "We score the compiled agent on the validation split, inspect per-example predictions, and print the reasoning trace for one ticket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd433d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_examples = val_examples if val_examples else train_examples\n",
    "scores: List[float] = []\n",
    "feedback_snippets: List[str] = []\n",
    "\n",
    "for example in eval_examples:\n",
    "    prediction = compiled_reasoner(**example.inputs())\n",
    "    score_feedback = routing_feedback_metric(example, prediction)\n",
    "    scores.append(score_feedback.score)\n",
    "    feedback_snippets.append(score_feedback.feedback.splitlines()[0])\n",
    "\n",
    "mean_score = sum(scores) / len(scores) if scores else 0.0\n",
    "print(f\"Validation examples: {len(eval_examples)} | Mean routing metric: {mean_score:.2f}\")\n",
    "\n",
    "if eval_examples:\n",
    "    sample = eval_examples[0]\n",
    "    sample_prediction = compiled_reasoner(**sample.inputs())\n",
    "    sample_feedback = routing_feedback_metric(sample, sample_prediction)\n",
    "\n",
    "    print(\"\\nSample Task:\\n\", sample.task)\n",
    "    print(\"\\nPrediction:\")\n",
    "    print(f\"Assigned To: {getattr(sample_prediction, 'assigned_to', [])}\")\n",
    "    print(f\"Execution Mode: {getattr(sample_prediction, 'execution_mode', 'delegated')}\")\n",
    "    print(f\"Tool Plan: {getattr(sample_prediction, 'tool_plan', [])}\")\n",
    "    print(f\"Score: {sample_feedback.score:.2f}\")\n",
    "\n",
    "    snippet = \"\\n\".join(sample_feedback.feedback.splitlines()[:8])\n",
    "    print(\"\\nFeedback:\\n\", snippet)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AgenticFleet (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
