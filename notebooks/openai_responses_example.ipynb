{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a8ef6e9",
   "metadata": {},
   "source": [
    "# OpenAI Responses Client Direct Usage Example\n",
    "\n",
    "This notebook demonstrates direct `OpenAIResponsesClient` usage for structured response generation with OpenAI models.\n",
    "\n",
    "**Features:**\n",
    "- Function calling capabilities with custom business logic\n",
    "- Structured output generation with Pydantic models\n",
    "- Streaming and non-streaming response modes\n",
    "\n",
    "**Prerequisites:**\n",
    "- Set `OPENAI_API_KEY` and `OPENAI_RESPONSES_MODEL_ID` environment variables (optionally configure `OPENAI_BASE_URL` and `OPENAI_ORG_ID`)\n",
    "- Install the `agent-framework` Python package and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c410494f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import os\n",
    "from random import randint\n",
    "from typing import Annotated\n",
    "\n",
    "from agent_framework import ChatResponse\n",
    "from agent_framework.exceptions import ServiceResponseException\n",
    "from agent_framework.openai import OpenAIResponsesClient\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "REQUIRED_ENV_VARS = [\"OPENAI_API_KEY\", \"OPENAI_RESPONSES_MODEL_ID\"]\n",
    "missing_env_vars = [var for var in REQUIRED_ENV_VARS if not os.environ.get(var)]\n",
    "if missing_env_vars:\n",
    "    raise OSError(\"Missing required environment variables: \" + \", \".join(missing_env_vars))\n",
    "\n",
    "OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]\n",
    "OPENAI_RESPONSES_MODEL_ID = os.environ[\"OPENAI_RESPONSES_MODEL_ID\"]\n",
    "OPENAI_BASE_URL = os.environ.get(\"OPENAI_BASE_URL\", \"\").strip()\n",
    "OPENAI_ORG_ID = os.environ.get(\"OPENAI_ORG_ID\", \"\").strip()\n",
    "\n",
    "\n",
    "def build_responses_client() -> OpenAIResponsesClient:\n",
    "    \"\"\"Create an OpenAI Responses client using environment configuration.\"\"\"\n",
    "    return OpenAIResponsesClient(\n",
    "        api_key=OPENAI_API_KEY,\n",
    "        model_id=OPENAI_RESPONSES_MODEL_ID,\n",
    "        base_url=OPENAI_BASE_URL or None,\n",
    "        org_id=OPENAI_ORG_ID or None,\n",
    "    )\n",
    "\n",
    "\n",
    "def handle_service_exception(context: str, exc: ServiceResponseException) -> None:\n",
    "    \"\"\"Provide actionable guidance when OpenAI returns an error.\"\"\"\n",
    "    error_text = str(exc)\n",
    "    print(f\"{context} failed: {error_text}\")\n",
    "    if \"404\" in error_text:\n",
    "        print(\n",
    "            \"Hint: The requested model may be unavailable. Verify OPENAI_RESPONSES_MODEL_ID matches an enabled Responses model.\"\n",
    "        )\n",
    "    elif \"401\" in error_text:\n",
    "        print(\"Hint: OPENAI_API_KEY may be invalid or missing. Regenerate the key and try again.\")\n",
    "    elif \"429\" in error_text:\n",
    "        print(\n",
    "            \"Hint: The request hit rate or quota limits. Reduce throughput or request a higher rate limit.\"\n",
    "        )\n",
    "\n",
    "\n",
    "client = build_responses_client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88379429",
   "metadata": {},
   "source": [
    "## Define Custom Tools\n",
    "\n",
    "Create a simple weather tool that the agent can call to get weather information for different locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7426651f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weather(\n",
    "    location: Annotated[str, Field(description=\"The location to get the weather for.\")],\n",
    ") -> str:\n",
    "    \"\"\"Get the weather for a given location.\"\"\"\n",
    "    conditions = [\"sunny\", \"cloudy\", \"rainy\", \"stormy\"]\n",
    "    return f\"The weather in {location} is {conditions[randint(0, 3)]} with a high of {randint(10, 30)} degrees Celsius.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7383583c",
   "metadata": {},
   "source": [
    "## Define Structured Output Format\n",
    "\n",
    "Use Pydantic models to define the expected structure of the agent's response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df2acfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutputStruct(BaseModel):\n",
    "    \"\"\"Structured output for weather information.\"\"\"\n",
    "\n",
    "    location: str\n",
    "    weather: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966149b5",
   "metadata": {},
   "source": [
    "## Initialize the Client and Get Response\n",
    "\n",
    "Create an OpenAI Responses Client and query for weather information with structured output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99b1a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the configuration being used for clarity\n",
    "configured_base_url = getattr(client, \"base_url\", None)\n",
    "if not configured_base_url or configured_base_url == \"None\":\n",
    "    configured_base_url = \"https://qredence-foundry.openai.azure.com/openai/v1\"\n",
    "print(f\"Configured model: {client.model_id}\")\n",
    "print(f\"Using base URL: {configured_base_url}\")\n",
    "\n",
    "# Define the user message\n",
    "message = \"What's the weather in Amsterdam and in Paris?\"\n",
    "print(f\"User: {message}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bd1767",
   "metadata": {},
   "source": [
    "## Option 1: Streaming Response\n",
    "\n",
    "Get a streaming response from the model, which provides real-time output as it's generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947070c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get streaming response\n",
    "try:\n",
    "    response = await ChatResponse.from_chat_response_generator(\n",
    "        client.get_streaming_response(message, tools=get_weather, response_format=OutputStruct),\n",
    "        output_format_type=OutputStruct,\n",
    "    )\n",
    "    print(f\"Assistant: {response.value}\")\n",
    "except ServiceResponseException as exc:\n",
    "    handle_service_exception(\"Streaming response\", exc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c82aad6",
   "metadata": {},
   "source": [
    "## Option 2: Non-Streaming Response\n",
    "\n",
    "Get a complete response in a single call without streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e6395c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get non-streaming response\n",
    "try:\n",
    "    response = await client.get_response(message, tools=get_weather, response_format=OutputStruct)\n",
    "    print(f\"Assistant: {response.value}\")\n",
    "except ServiceResponseException as exc:\n",
    "    handle_service_exception(\"Non-streaming response\", exc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentic-fleet (3.12.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
