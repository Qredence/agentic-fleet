{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50ed36a0",
   "metadata": {},
   "source": [
    "# AgenticFleet Fleet Support GEPA Notebook\n",
    "\n",
    "This notebook adapts the [DSPy GEPA Facility Support Analyzer tutorial](https://dspy.ai/tutorials/gepa_facilitysupportanalyzer/) to the AgenticFleet codebase. We will walk through defining a fleet-support-specific signature, building a custom DSPy module, preparing training data, compiling with `BootstrapFewShot`, and evaluating the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376877c1",
   "metadata": {},
   "source": [
    "## 1. Import Dependencies and Configure DSPy\n",
    "\n",
    "We load project utilities, configure Python paths, and initialize DSPy's LM backend with the same model referenced in `config/workflow_config.yaml` (`gpt-4.1`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb4d3b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Volumes/Samsung-SSD-T7/Workspaces/Github/qredence/agent-framework/v0.5/AgenticFleet\n",
      "Configured DSPy with model: gpt-4.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "import inspect\n",
    "from pathlib import Path\n",
    "from types import SimpleNamespace\n",
    "from typing import List\n",
    "\n",
    "# Ensure repo src/ is on the path for absolute imports\n",
    "# Use parent of notebooks/ to get the project root\n",
    "PROJECT_ROOT = Path(__file__).resolve().parent.parent if \"__file__\" in dir() else Path(\".\").resolve().parent\n",
    "SRC_PATH = PROJECT_ROOT / \"src\"\n",
    "if str(SRC_PATH) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC_PATH))\n",
    "\n",
    "import dspy\n",
    "from agentic_fleet.utils.config_loader import load_config\n",
    "from agentic_fleet.dspy_modules.reasoner import DSPyReasoner\n",
    "from agentic_fleet.dspy_modules.workflow_signatures import EnhancedTaskRouting\n",
    "from agentic_fleet.utils.gepa_optimizer import (\n",
    "    load_example_dicts,\n",
    "    prepare_gepa_datasets,\n",
    "    build_routing_feedback_metric,\n",
    ")\n",
    "\n",
    "# Configure DSPy using the model defined in workflow config (defaults to gpt-4.1)\n",
    "workflow_config = load_config()\n",
    "dspy_model = workflow_config.get(\"dspy\", {}).get(\"model\", \"gpt-4.1\")\n",
    "\n",
    "lm = dspy.LM(model=dspy_model, max_tokens=1024)\n",
    "dspy.configure(lm=lm)\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Configured DSPy with model: {dspy_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877c9e27",
   "metadata": {},
   "source": [
    "## 2. Define the Fleet Support Signature\n",
    "\n",
    "AgenticFleet already ships with an enhanced routing signature (`EnhancedTaskRouting`) in `src/agentic_fleet/dspy_modules/workflow_signatures.py`. We inspect it directly so the notebook stays aligned with the production supervisor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85f78a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EnhancedTaskRouting signature (source excerpt):\n",
      "\n",
      "class EnhancedTaskRouting(dspy.Signature):\n",
      "    \"\"\"Advanced task routing with efficiency and tool-planning awareness.\n",
      "\n",
      "    Optimizes for latency and token usage by pre-planning tool usage\n",
      "    and setting execution constraints.\n",
      "    \"\"\"\n",
      "\n",
      "    task: str = dspy.InputField(desc=\"Task to be routed\")\n",
      "    team_capabilities: str = dspy.InputField(desc=\"Capabilities of available agents\")\n",
      "    available_tools: str = dspy.InputField(desc=\"List of available tools\")\n",
      "    current_context: str = dspy.InputField(desc=\"Execution context\")\n",
      "    handoff_history: str = dspy.InputField(desc=\"History of agent handoffs\")\n",
      "    workflow_state: str = dspy.InputField(desc=\"Current state of the workflow\")\n",
      "\n",
      "    assigned_to: list[str] = dspy.OutputField(desc=\"Agents assigned to the task\")\n",
      "    execution_mode: Literal[\"delegated\", \"sequential\", \"parallel\"] = dspy.OutputField(\n",
      "        desc=\"Execution mode\"\n",
      "    )\n",
      "    subtasks: list[str] = dspy.OutputField(desc=\"Breakdown of subtasks\")\n",
      "\n",
      "    handoff_strategy: str = dspy.OutputField(desc=\"Strategy for agent handoffs\")\n",
      "    workflow_gates: str = dspy.OutputField(desc=\"Quality gates and checkpoints\")\n",
      "\n",
      "    # Efficiency and Tool Planning Fields\n",
      "    tool_plan: list[str] = dspy.OutputField(desc=\"Ordered list of tools to use\")\n",
      "    tool_goals: str = dspy.OutputField(desc=\"Specific goals for tool usage\")\n",
      "    latency_budget: str = dspy.OutputField(desc=\"Estimated time/latency budget\")\n",
      "    reasoning: str = dspy.OutputField(desc=\"Reasoning for the routing decision\")\n",
      "\n",
      "\n",
      "Input fields:\n",
      "- task: Task to be routed\n",
      "- team_capabilities: Capabilities of available agents\n",
      "- available_tools: List of available tools\n",
      "- current_context: Execution context\n",
      "- handoff_history: History of agent handoffs\n",
      "- workflow_state: Current state of the workflow\n",
      "\n",
      "Output fields:\n",
      "- assigned_to: Agents assigned to the task\n",
      "- execution_mode: Execution mode\n",
      "- subtasks: Breakdown of subtasks\n",
      "- handoff_strategy: Strategy for agent handoffs\n",
      "- workflow_gates: Quality gates and checkpoints\n",
      "- tool_plan: Ordered list of tools to use\n",
      "- tool_goals: Specific goals for tool usage\n",
      "- latency_budget: Estimated time/latency budget\n",
      "- reasoning: Reasoning for the routing decision\n"
     ]
    }
   ],
   "source": [
    "print(\"EnhancedTaskRouting signature (source excerpt):\\n\")\n",
    "print(inspect.getsource(EnhancedTaskRouting))\n",
    "\n",
    "print(\"\\nInput fields:\")\n",
    "for name, field in EnhancedTaskRouting.input_fields.items():\n",
    "    print(f\"- {name}: {field.json_schema_extra.get('desc', 'No description')}\")\n",
    "\n",
    "print(\"\\nOutput fields:\")\n",
    "for name, field in EnhancedTaskRouting.output_fields.items():\n",
    "    print(f\"- {name}: {field.json_schema_extra.get('desc', 'No description')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afa9cc8",
   "metadata": {},
   "source": [
    "## 3. Create the Agentic Fleet Module\n",
    "\n",
    "We reuse the production `DSPyReasoner` module (the same class that powers the Supervisor workflow). It wires multiple DSPy submodules‚Äîtask analysis, routing, progress, and tool planning‚Äîunder one interface that GEPA can optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28a0386a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named predictors exposed to GEPA:\n",
      "['analyzer', 'router', 'quality_assessor', 'progress_evaluator', 'tool_planner', 'simple_responder', 'group_chat_selector', 'strategy_selector']\n",
      "\n",
      "Sample routing decision:\n",
      "{'assigned_to': ['Planner', 'Researcher', 'Writer'], 'execution_mode': 'sequential', 'tool_plan': ['TavilySearchTool', 'HostedCodeInterpreterTool']}\n"
     ]
    }
   ],
   "source": [
    "reasoner = DSPyReasoner(use_enhanced_signatures=True)\n",
    "print(\"Named predictors exposed to GEPA:\")\n",
    "print([name for name, _ in reasoner.named_predictors()])\n",
    "\n",
    "# Quick smoke test using a lightweight task routed through the router signature\n",
    "sample_prediction = reasoner(\n",
    "    task=\"Summarize the latest GEPA optimizations for the exec weekly report\",\n",
    "    team_capabilities=(\n",
    "        \"Planner: decomposes projects.\\n\"\n",
    "        \"Researcher: runs Tavily searches.\\n\"\n",
    "        \"Writer: drafts polished updates.\"\n",
    "    ),\n",
    "    available_tools=\"TavilySearchTool, HostedCodeInterpreterTool\",\n",
    "    current_context=\"Need executive-friendly tone, highlight latency gains.\",\n",
    ")\n",
    "\n",
    "print(\"\\nSample routing decision:\")\n",
    "print(\n",
    "    {\n",
    "        \"assigned_to\": getattr(sample_prediction, \"assigned_to\", []),\n",
    "        \"execution_mode\": getattr(sample_prediction, \"execution_mode\", \"delegated\"),\n",
    "        \"tool_plan\": getattr(sample_prediction, \"tool_plan\", []),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12995b3",
   "metadata": {},
   "source": [
    "## 4. Load and Prepare Fleet Support Data\n",
    "\n",
    "Supervisor training data now lives exclusively in `src/agentic_fleet/data/supervisor_examples.json`. Use `scripts/merge_supervisor_examples.py` if you need to ingest additional examples (it will merge any extra files and regenerate the canonical dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c48011b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training examples: 87 | Validation examples: 21\n",
      "\n",
      "Example record:\n",
      "{'task': 'Write a technical blog post about machine learning trends', 'assigned_to': 'Researcher,Writer,Reviewer', 'execution_mode': 'sequential', 'tool_requirements': ['TavilySearchTool']}\n"
     ]
    }
   ],
   "source": [
    "examples_path = PROJECT_ROOT / \"src\" / \"agentic_fleet\" / \"data\" / \"supervisor_examples.json\"\n",
    "records = load_example_dicts(str(examples_path))\n",
    "\n",
    "if not records:\n",
    "    raise RuntimeError(\n",
    "        \"No training data found. Run scripts/merge_supervisor_examples.py to populate the dataset.\"\n",
    "    )\n",
    "\n",
    "train_examples, val_examples = prepare_gepa_datasets(\n",
    "    base_examples_path=str(examples_path),\n",
    "    base_records=records,\n",
    "    val_split=0.2,\n",
    "    seed=13,\n",
    ")\n",
    "\n",
    "print(f\"Training examples: {len(train_examples)} | Validation examples: {len(val_examples)}\")\n",
    "\n",
    "print(\"\\nExample record:\")\n",
    "sample = train_examples[0]\n",
    "print({\n",
    "    \"task\": sample.task,\n",
    "    \"assigned_to\": sample.assigned_to,\n",
    "    \"execution_mode\": sample.execution_mode,\n",
    "    \"tool_requirements\": sample.tool_requirements,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0898d1",
   "metadata": {},
   "source": [
    "## 5. Define Evaluation Metrics\n",
    "\n",
    "Our metric encourages the agent to produce correct diagnoses and actionable plans. We score partial credit for overlapping agent/tool selections so GEPA can receive granular feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2790bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric ready ‚Äì returns GEPA-friendly score + feedback text.\n",
      "Score: 0.17\n",
      "\n",
      "Feedback excerpt:\n",
      "\n",
      "‚ùå Routing decision needs significant improvement.\n",
      "\n",
      "üîç Edge Cases Detected:\n",
      "  ‚Ä¢ Edge case: Task requires multiple agents but was assigned to fewer. Consider task complexity and required capabilities.\n",
      "\n",
      "üìä Component Analysis:\n",
      "  ‚ùå Agent mismatch: Assigned ['Writer'] but expected ['Researcher', 'Writer', 'Reviewer'].\n",
      "  üìù Step-by-step: First, analyze task requirements. Then, match capabilities:\n"
     ]
    }
   ],
   "source": [
    "routing_feedback_metric = build_routing_feedback_metric()\n",
    "print(\"Metric ready ‚Äì returns GEPA-friendly score + feedback text.\")\n",
    "\n",
    "# Demonstrate feedback using a real training example and a deliberately bad prediction\n",
    "gold = train_examples[0]\n",
    "bad_prediction = SimpleNamespace(\n",
    "    task=gold.task,\n",
    "    assigned_to=[\"Writer\"],  # intentionally wrong agent\n",
    "    execution_mode=\"delegated\",\n",
    "    tool_requirements=[],\n",
    "    latency_budget=\"low\",\n",
    ")\n",
    "\n",
    "score_feedback = routing_feedback_metric(gold, bad_prediction)\n",
    "print(f\"Score: {score_feedback.score:.2f}\\n\")\n",
    "print(\"Feedback excerpt:\\n\")\n",
    "print(\"\\n\".join(score_feedback.feedback.splitlines()[:8]))\n",
    "\n",
    "\n",
    "def bootstrap_routing_metric(example, prediction, trace=None):\n",
    "    \"\"\"Lightweight scorer used by BootstrapFewShot (mirrors compiler.py).\"\"\"\n",
    "    gold_agents = {agent.strip().lower() for agent in str(example.assigned_to).split(\",\") if agent}\n",
    "    pred_agents = {agent.strip().lower() for agent in getattr(prediction, \"assigned_to\", [])}\n",
    "    assignment_score = 1.0 if gold_agents & pred_agents else 0.0\n",
    "    mode_score = 1.0 if getattr(prediction, \"execution_mode\", \"\") == example.execution_mode else 0.0\n",
    "    return (assignment_score * 0.7) + (mode_score * 0.3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abab793",
   "metadata": {},
   "source": [
    "## 6. Compile the Agent with BootstrapFewShot\n",
    "\n",
    "We warm-start the fleet module with DSPy's `BootstrapFewShot` teleprompter so the LM observes a few good demonstrations before GEPA fine-tuning. This mirrors the standard AgenticFleet optimization pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a29ae4d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BootstrapFewShot compilation against supervisor dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|‚ñã         | 6/87 [00:19<04:26,  3.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 6 full traces after 6 examples for up to 1 rounds, amounting to 6 attempts.\n",
      "Compilation complete. You can now hand this module to GEPA for further tuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from dspy.teleprompt import BootstrapFewShot\n",
    "\n",
    "max_demos = min(6, len(train_examples)) or 1\n",
    "teleprompter = BootstrapFewShot(\n",
    "    metric=bootstrap_routing_metric,\n",
    "    max_bootstrapped_demos=max_demos,\n",
    "    max_labeled_demos=max_demos,\n",
    ")\n",
    "\n",
    "print(\"Running BootstrapFewShot compilation against supervisor dataset...\")\n",
    "compiled_reasoner = teleprompter.compile(reasoner, trainset=train_examples)\n",
    "print(\"Compilation complete. You can now hand this module to GEPA for further tuning.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a3066f",
   "metadata": {},
   "source": [
    "## 7. Evaluate and Inspect Results\n",
    "\n",
    "We score the compiled agent on the validation split, inspect per-example predictions, and print the reasoning trace for one ticket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd433d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation examples: 21 | Mean routing metric: 0.81\n",
      "\n",
      "Sample Task:\n",
      " Who won the 2025 New York mayor election? Search for Zohran Mamdani and provide details about the winner.\n",
      "\n",
      "Prediction:\n",
      "Assigned To: ['Researcher', 'Writer']\n",
      "Execution Mode: sequential\n",
      "Tool Plan: ['TavilySearchTool']\n",
      "Score: 0.80\n",
      "\n",
      "Feedback:\n",
      " ‚ö†Ô∏è Routing decision is mostly correct but has minor issues.\n",
      "\n",
      "üîç Edge Cases Detected:\n",
      "  ‚Ä¢ This task involves ambiguity - consider clarifying requirements before routing.\n",
      "  ‚Ä¢ Edge case: Time-sensitive query detected but web search tool not assigned. Tasks about current events, latest data, or future dates require TavilySearchTool.\n",
      "\n",
      "üìä Component Analysis:\n",
      "  ‚úÖ Agent selection matches ground truth.\n"
     ]
    }
   ],
   "source": [
    "eval_examples = val_examples if val_examples else train_examples\n",
    "scores: List[float] = []\n",
    "feedback_snippets: List[str] = []\n",
    "\n",
    "for example in eval_examples:\n",
    "    prediction = compiled_reasoner(**example.inputs())\n",
    "    score_feedback = routing_feedback_metric(example, prediction)\n",
    "    scores.append(score_feedback.score)\n",
    "    feedback_snippets.append(score_feedback.feedback.splitlines()[0])\n",
    "\n",
    "mean_score = sum(scores) / len(scores) if scores else 0.0\n",
    "print(f\"Validation examples: {len(eval_examples)} | Mean routing metric: {mean_score:.2f}\")\n",
    "\n",
    "if eval_examples:\n",
    "    sample = eval_examples[0]\n",
    "    sample_prediction = compiled_reasoner(**sample.inputs())\n",
    "    sample_feedback = routing_feedback_metric(sample, sample_prediction)\n",
    "\n",
    "    print(\"\\nSample Task:\\n\", sample.task)\n",
    "    print(\"\\nPrediction:\")\n",
    "    print(f\"Assigned To: {getattr(sample_prediction, 'assigned_to', [])}\")\n",
    "    print(f\"Execution Mode: {getattr(sample_prediction, 'execution_mode', 'delegated')}\")\n",
    "    print(f\"Tool Plan: {getattr(sample_prediction, 'tool_plan', [])}\")\n",
    "    print(f\"Score: {sample_feedback.score:.2f}\")\n",
    "\n",
    "    snippet = \"\\n\".join(sample_feedback.feedback.splitlines()[:8])\n",
    "    print(\"\\nFeedback:\\n\", snippet)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AgenticFleet (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
