{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1186beb",
   "metadata": {},
   "source": [
    "# Magentic Orchestration Deep Dive\n",
    "\n",
    "Explore how AgenticFleet layers Microsoft Agent Framework's Magentic workflow across planning, delegation, streaming telemetry, plan review, and checkpoint resume capabilities. Follow the sections below to wire up a notebook-friendly fleet, execute a task, then inspect every artifact the orchestrator produces.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217891e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2713 Imports loaded successfully\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any\n",
    "\n",
    "try:\n",
    "    import agent_framework  # noqa: F401\n",
    "except ModuleNotFoundError as exc:  # pragma: no cover - environment guard\n",
    "    raise RuntimeError(\n",
    "        \"This notebook requires the `agent-framework` package. \"\n",
    "        \"Install it with `uv add agent-framework`.\"\n",
    "    ) from exc\n",
    "\n",
    "from agenticfleet.cli.ui import AgentMessage, FinalRenderData\n",
    "from agenticfleet.config import settings\n",
    "from agenticfleet.core.approval import (\n",
    "    ApprovalDecision,\n",
    "    ApprovalHandler,\n",
    "    ApprovalRequest,\n",
    "    ApprovalResponse,\n",
    ")\n",
    "from agenticfleet.core.approved_tools import set_approval_handler\n",
    "from agenticfleet.fleet.callbacks import ConsoleCallbacks\n",
    "from agenticfleet.fleet.magentic_fleet import MagenticFleet\n",
    "\n",
    "if not settings.openai_api_key:\n",
    "    print(\n",
    "        \"WARNING: OPENAI_API_KEY is not configured. Hosted model calls will fail until it is set.\"\n",
    "    )\n",
    "\n",
    "print(\"\u2713 Core imports loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0db7954",
   "metadata": {},
   "source": [
    "## Notebook observer utilities\n",
    "\n",
    "Collect Magentic callbacks in-memory so we can review task plans, progress ledgers, agent responses, and final answers after each run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b094c4b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2713 Researcher agent configured\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class NotebookUI:\n",
    "    plan_history: list[dict[str, list[str]]] = field(default_factory=list)\n",
    "    progress_history: list[dict[str, Any]] = field(default_factory=list)\n",
    "    agent_messages: list[AgentMessage] = field(default_factory=list)\n",
    "    notices: list[dict[str, str]] = field(default_factory=list)\n",
    "    final_payloads: list[dict[str, Any]] = field(default_factory=list)\n",
    "\n",
    "    @staticmethod\n",
    "    def _to_lines(value: Any) -> list[str]:\n",
    "        if value is None:\n",
    "            return []\n",
    "        if isinstance(value, str):\n",
    "            return [line.strip() for line in value.splitlines() if line.strip()]\n",
    "        if isinstance(value, dict):\n",
    "            return [f\"{key}: {val}\" for key, val in value.items()]\n",
    "        if isinstance(value, (list, tuple, set)):\n",
    "            lines: list[str] = []\n",
    "            for item in value:\n",
    "                lines.extend(NotebookUI._to_lines(item))\n",
    "            return lines\n",
    "        text = str(value).strip()\n",
    "        return [text] if text else []\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        self.plan_history.clear()\n",
    "        self.progress_history.clear()\n",
    "        self.agent_messages.clear()\n",
    "        self.notices.clear()\n",
    "        self.final_payloads.clear()\n",
    "\n",
    "    def log_plan(\n",
    "        self, facts: list[str] | tuple[str, ...] | None, plan: list[str] | tuple[str, ...] | None\n",
    "    ) -> None:\n",
    "        self.plan_history.append(\n",
    "            {\n",
    "                \"facts\": self._to_lines(facts),\n",
    "                \"plan\": self._to_lines(plan),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def log_progress(self, status: str, next_speaker: str, instruction: str | None = None) -> None:\n",
    "        self.progress_history.append(\n",
    "            {\n",
    "                \"status\": status,\n",
    "                \"next_speaker\": next_speaker,\n",
    "                \"instruction\": self._to_lines(instruction),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def log_agent_message(self, message: AgentMessage) -> None:\n",
    "        self.agent_messages.append(\n",
    "            AgentMessage(agent_name=message.agent_name, content=message.content, mode=message.mode)\n",
    "        )\n",
    "\n",
    "    def log_notice(self, text: str, *, style: str = \"blue\") -> None:\n",
    "        self.notices.append({\"text\": text, \"style\": style})\n",
    "\n",
    "    def log_final(self, result: FinalRenderData | str | Any | None) -> None:\n",
    "        if isinstance(result, FinalRenderData):\n",
    "            payload = {\"sections\": result.sections, \"raw_text\": result.raw_text}\n",
    "        elif isinstance(result, str) or result is None:\n",
    "            payload = {\"raw_text\": result}\n",
    "        else:\n",
    "            payload = {\"raw_object\": str(result)}\n",
    "        self.final_payloads.append(payload)\n",
    "\n",
    "    def latest_summary(self) -> dict[str, Any]:\n",
    "        return {\n",
    "            \"plans\": self.plan_history,\n",
    "            \"progress\": self.progress_history,\n",
    "            \"agents\": [\n",
    "                {\"agent\": message.agent_name, \"content\": message.content}\n",
    "                for message in self.agent_messages\n",
    "            ],\n",
    "            \"notices\": self.notices,\n",
    "            \"final\": self.final_payloads,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846d9952",
   "metadata": {},
   "source": [
    "## Human-in-the-loop policy for notebooks\n",
    "\n",
    "Magentic fleets expect an approval handler when HITL is enabled. We use a lightweight handler that auto-approves tool calls but rewrites the first plan to show how plan review feedback alters execution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d300f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2713 Coder agent configured\n"
     ]
    }
   ],
   "source": [
    "class NotebookApprovalHandler(ApprovalHandler):\n",
    "    def __init__(self, *, add_plan_revision: bool = True) -> None:\n",
    "        self.add_plan_revision = add_plan_revision\n",
    "        self.history: list[ApprovalResponse] = []\n",
    "        self._plan_review_round = 0\n",
    "\n",
    "    async def request_approval(self, request: ApprovalRequest) -> ApprovalResponse:\n",
    "        if request.operation_type == \"plan_review\" and self.add_plan_revision:\n",
    "            self._plan_review_round += 1\n",
    "            if self._plan_review_round == 1:\n",
    "                plan_text = \"\"\n",
    "                if request.details:\n",
    "                    plan_text = str(request.details.get(\"plan_text\", \"\")).strip()\n",
    "                plan_lines = NotebookUI._to_lines(plan_text) or []\n",
    "                if \"Validate energy baselines before final answer.\" not in plan_lines:\n",
    "                    plan_lines.append(\"Validate energy baselines before final answer.\")\n",
    "                revised_plan = \"\\n\".join(plan_lines)\n",
    "                response = ApprovalResponse(\n",
    "                    request_id=request.request_id,\n",
    "                    decision=ApprovalDecision.MODIFIED,\n",
    "                    modified_code=revised_plan,\n",
    "                    reason=\"Notebook approval inserted a validation step into the plan.\",\n",
    "                )\n",
    "            else:\n",
    "                response = ApprovalResponse(\n",
    "                    request_id=request.request_id,\n",
    "                    decision=ApprovalDecision.APPROVED,\n",
    "                    modified_code=None,\n",
    "                    reason=\"Subsequent plans approved without changes.\",\n",
    "                )\n",
    "        else:\n",
    "            response = ApprovalResponse(\n",
    "                request_id=request.request_id,\n",
    "                decision=ApprovalDecision.APPROVED,\n",
    "                modified_code=None,\n",
    "                reason=\"Auto-approved for notebook demo.\",\n",
    "            )\n",
    "        self.history.append(response)\n",
    "        return response\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        self.history.clear()\n",
    "        self._plan_review_round = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44f0486",
   "metadata": {},
   "source": [
    "## Build a notebook-friendly Magentic fleet\n",
    "\n",
    "Create the fleet with default agents, wire our observer UI into the callback pipeline, and register the auto-approval policy. Checkpointing follows the repository configuration, so you can resume from prior runs when storage is enabled.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcabe8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2713 Event callback defined\n"
     ]
    }
   ],
   "source": [
    "notebook_ui = NotebookUI()\n",
    "approval_handler = NotebookApprovalHandler()\n",
    "\n",
    "workflow_config = settings.workflow_config.get(\"workflow\", {}) or {}\n",
    "hitl_config = workflow_config.get(\"human_in_the_loop\", {}) or {}\n",
    "set_approval_handler(\n",
    "    approval_handler,\n",
    "    require_operations=hitl_config.get(\"require_approval_for\", []),\n",
    "    trusted_operations=hitl_config.get(\"trusted_operations\", []),\n",
    ")\n",
    "\n",
    "checkpoint_storage = settings.create_checkpoint_storage()\n",
    "\n",
    "fleet = MagenticFleet(\n",
    "    checkpoint_storage=checkpoint_storage,\n",
    "    approval_handler=approval_handler,\n",
    "    approval_policy=hitl_config,\n",
    "    console_callbacks=ConsoleCallbacks(notebook_ui),\n",
    ")\n",
    "\n",
    "storage_label = type(checkpoint_storage).__name__ if checkpoint_storage else \"disabled\"\n",
    "print(f\"\u2713 Fleet ready (checkpoint storage: {storage_label})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fada32ea",
   "metadata": {},
   "source": [
    "## Inspect workflow configuration\n",
    "\n",
    "Summarize the active Magentic settings so we know which features are available during the run (manager model, plan review, HITL, and checkpoint persistence).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef65500",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-20 06:54:52 - /Volumes/Samsung-SSD-T7/Workspaces/Github/qredence/agent-framework/v0.5/AgenticFleet/.venv/lib/python3.12/site-packages/agent_framework/_workflows/_validation.py:520 - WARNING] Cycle detected in the workflow graph involving: agent_coder -> agent_researcher -> magentic_orchestrator -> agent_coder. Ensure termination or iteration limits exist.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Magentic workflow...\n",
      "\u2713 Workflow built successfully\n"
     ]
    }
   ],
   "source": [
    "fleet_config = settings.workflow_config.get(\"fleet\", {}) or {}\n",
    "checkpoint_config = (\n",
    "    workflow_config.get(\"checkpointing\", {}) if isinstance(workflow_config, dict) else {}\n",
    ")\n",
    "\n",
    "config_summary = {\n",
    "    \"manager_model\": fleet_config.get(\"manager\", {}).get(\"model\"),\n",
    "    \"plan_review_enabled\": fleet_config.get(\"plan_review\", {}).get(\"enabled\"),\n",
    "    \"orchestrator_limits\": {\n",
    "        \"max_rounds\": fleet_config.get(\"orchestrator\", {}).get(\"max_round_count\"),\n",
    "        \"max_stalls\": fleet_config.get(\"orchestrator\", {}).get(\"max_stall_count\"),\n",
    "        \"max_resets\": fleet_config.get(\"orchestrator\", {}).get(\"max_reset_count\"),\n",
    "    },\n",
    "    \"hitl\": {\n",
    "        \"enabled\": hitl_config.get(\"enabled\"),\n",
    "        \"requires\": hitl_config.get(\"require_approval_for\", []),\n",
    "        \"trusted\": hitl_config.get(\"trusted_operations\", []),\n",
    "    },\n",
    "    \"checkpointing\": {\n",
    "        \"enabled\": checkpoint_config.get(\"enabled\"),\n",
    "        \"storage_type\": checkpoint_config.get(\"storage_type\"),\n",
    "        \"storage_path\": checkpoint_config.get(\"storage_path\"),\n",
    "        \"active_storage\": type(checkpoint_storage).__name__ if checkpoint_storage else \"disabled\",\n",
    "    },\n",
    "}\n",
    "\n",
    "print(json.dumps(config_summary, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a96b48",
   "metadata": {},
   "source": [
    "## Define a sample orchestration task\n",
    "\n",
    "Use an energy-efficiency comparison prompt that forces the researcher, coder, and analyst to collaborate. The task is identical to the CLI demo so checkpoints can be reused between environments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6b32cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2713 Task defined: I am preparing a report on the energy efficiency of different machine learning model architectures. ...\n"
     ]
    }
   ],
   "source": [
    "TASK_REQUEST = (\n",
    "    \"I am preparing a report on the energy efficiency of different \"\n",
    "    \"machine learning model architectures. \"\n",
    "    \"Compare the estimated training and inference energy \"\n",
    "    \"consumption of ResNet-50, BERT-base, and GPT-2 on \"\n",
    "    \"standard datasets (e.g., ImageNet for ResNet, GLUE for BERT, \"\n",
    "    \"WebText for GPT-2). Estimate the CO2 emissions for each \"\n",
    "    \"model assuming training on an Azure Standard_NC6s_v3 VM \"\n",
    "    \"for 24 hours, and surface the assumptions you make. \"\n",
    "    \"Provide tables for clarity, recommend the most \"\n",
    "    \"energy-efficient model per task type (image classification, \"\n",
    "    \"text classification, text generation), and call out any data \"\n",
    "    \"gaps that should be verified.\"\n",
    ")\n",
    "\n",
    "print(f\"\u2713 Task prepared ({len(TASK_REQUEST.split())} words)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f501a2",
   "metadata": {},
   "source": [
    "## Async helpers for running and summarizing the fleet\n",
    "\n",
    "The helpers below reset state before each run, invoke the Magentic workflow with optional checkpoint resume, and materialize a summary of every artifact captured by the notebook UI.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185bb6ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Starting workflow execution...\n",
      "==================================================\n",
      "\n",
      "\n",
      "[ORCH:user_task]\n",
      "\n",
      "I am preparing a report on the energy efficiency of different machine learning model architectures. Compare the estimated training and inference energy consumption of ResNet-50, BERT-base, and GPT-2 on standard datasets (e.g., ImageNet for ResNet, GLUE for BERT, WebText for GPT-2). Then, estimate the CO2 emissions associated with each, assuming training on an Azure Standard_NC6s_v3 VM for 24 hours. Provide tables for clarity, and recommend the most energy-efficient model per task type (image classification, text classification, and text generation).\n",
      "--------------------------\n",
      "\n",
      "[ORCH:task_ledger]\n",
      "\n",
      "\n",
      "We are working to address the following user request:\n",
      "\n",
      "I am preparing a report on the energy efficiency of different machine learning model architectures. Compare the estimated training and inference energy consumption of ResNet-50, BERT-base, and GPT-2 on standard datasets (e.g., ImageNet for ResNet, GLUE for BERT, WebText for GPT-2). Then, estimate the CO2 emissions associated with each, assuming training on an Azure Standard_NC6s_v3 VM for 24 hours. Provide tables for clarity, and recommend the most energy-efficient model per task type (image classification, text classification, and text generation).\n",
      "\n",
      "\n",
      "To answer this request we have assembled the following team:\n",
      "\n",
      "- researcher: Specialist in research and information gathering\n",
      "- coder: A helpful assistant that writes and executes code to process and analyze data.\n",
      "\n",
      "\n",
      "Here is an initial fact sheet to consider:\n",
      "\n",
      "1. GIVEN OR VERIFIED FACTS\n",
      "- Models to compare: ResNet-50, BERT\u2011base, GPT\u20112 (user-specified).\n",
      "- Standard datasets named: ImageNet (ResNet), GLUE (BERT), WebText (GPT\u20112) \u2014 user-specified.\n",
      "- Compute assumption: training on an Azure Standard_NC6s_v3 VM for 24 hours \u2014 user-specified.\n",
      "\n",
      "2. FACTS TO LOOK UP (and where)\n",
      "- Exact hardware/specs of Azure Standard_NC6s_v3 (GPU model, GPU memory, CPU, advertised TDP/power envelope): Azure VM sizes documentation (Microsoft Azure docs) and Azure NCv3 series pages.\n",
      "- Real-world power draw (idle vs. under full GPU load) for the NC6s_v3 or its GPU (likely NVIDIA Tesla V100): NVIDIA product/TDP specs, Microsoft docs, and independent benchmark articles (e.g., MLPerf/benchmark reports, cloud-provider power studies).\n",
      "- Typical data-center PUE for Azure or the relevant region: Microsoft Azure sustainability/green cloud reports or Azure sustainability dashboards.\n",
      "- Regional electricity carbon intensity (kg CO2 / kWh) for the Azure region in question: electricityMap, National grid data, IEA, or Azure Emissions Impact Dashboard (if region specified).\n",
      "- Empirical compute/time required to train each model on the stated datasets:\n",
      "  - ResNet\u201150 on ImageNet (training hours, number of GPUs, epoch counts): original ResNet paper (He et al.), common training recipes (e.g., PyTorch ImageNet scripts), and reproducibility reports (Papers With Code, timm training configs).\n",
      "  - BERT\u2011base training/fine-tuning on GLUE (compute hours for pretraining vs fine-tuning): Devlin et al. (BERT paper) for pretraining details, Strubell et al. 2019 and follow-ups for energy estimates, and Hugging Face / Transformers fine-tuning guides for GLUE.\n",
      "  - GPT\u20112 (117M) pretraining on WebText: OpenAI GPT\u20112 paper for training compute/steps; community reproductions and training logs for single-GPU estimates.\n",
      "- FLOPs (or MACs) per inference for each architecture and common sequence lengths: model papers, Papers With Code model card entries, and profiler tools (timm, transformers FLOP counters).\n",
      "- Inference latency and throughput profiles on V100 for these models: MLPerf, Hugging Face benchmarks, and cloud GPU instance benchmark articles.\n",
      "- Published lifecycle energy/CO2 estimates for comparable model trainings (for sanity checks): Strubell et al. 2019 (\"Energy and Policy Considerations for Deep Learning in NLP\"), Patterson et al. (2021) \"Carbon emissions and large neural network training\" or \"The Carbon Impact of AI\" literature.\n",
      "\n",
      "3. FACTS TO DERIVE\n",
      "- Energy consumed by training on NC6s_v3 for 24 hours: compute (power draw under expected utilization \u00d7 24 h \u00d7 PUE).\n",
      "- CO2 emissions for that 24\u2011hour run: energy (kWh) \u00d7 regional carbon intensity (kg CO2/kWh).\n",
      "- Per\u2011sample (or per\u2011token) inference energy: (device power \u00d7 inference latency \u00d7 PUE) or (FLOPs \u00d7 energy per FLOP estimate).\n",
      "- Aggregate inference energy for a dataset or traffic volume: per\u2011inference energy \u00d7 number of inferences.\n",
      "- Relative comparisons between models by normalizing energy per useful work metric (e.g., energy per validation accuracy point, energy per GLUE aggregate score point, or energy per token generated).\n",
      "\n",
      "4. EDUCATED GUESSES (memory / plausible defaults)\n",
      "- Model sizes (parameters):\n",
      "  - ResNet\u201150 \u2248 25 million parameters.\n",
      "  - BERT\u2011base \u2248 110 million parameters.\n",
      "  - GPT\u20112 (small, original) \u2248 117 million parameters.\n",
      "- FLOPs per inference (rough order of magnitude):\n",
      "  - ResNet\u201150 \u2248 4 GFLOPs per image.\n",
      "  - BERT\u2011base (sequence length 128) \u2248 tens of GFLOPs (often ~20\u201325 GFLOPs).\n",
      "  - GPT\u20112 (117M) per token \u2248 a few to ~10 GFLOPs depending on context length and implementation.\n",
      "- Typical GPU TDP and power draw:\n",
      "  - NVIDIA V100 TDP \u2248 250\u2013300 W; under full sustained ML load a single V100 often draws ~250\u2013300 W.\n",
      "- Typical data-center PUE: often in the 1.1\u20131.4 range; a common conservative default is ~1.2.\n",
      "- Default grid carbon intensity (if region unspecified): global average \u2248 0.4\u20130.5 kg CO2/kWh; many U.S. regions \u2248 0.3\u20130.6 kg CO2/kWh; low\u2011carbon regions can be <0.1 kg CO2/kWh.\n",
      "- Rough training time heuristics (single\u2011GPU, order-of-magnitude):\n",
      "  - ResNet\u201150 on ImageNet to reach standard accuracy on a single V100 can take multiple days; commonly reproduced multi\u2011GPU runs finish in hours (e.g., 8 GPUs \u00d7 ~12\u201324 hours for standard recipes). Single\u2011GPU 24 h likely yields a partial training run unless distributed.\n",
      "  - BERT\u2011base pretraining (from scratch) is high compute (many GPU-days); fine\u2011tuning on GLUE is much cheaper (minutes to a few hours on a single GPU).\n",
      "  - GPT\u20112 full pretraining on WebText is very expensive (many GPU-days); training on a single NC6s_v3 for 24 h would only cover a small fraction of full pretraining.\n",
      "\n",
      "(End of pre\u2011survey answers.)\n",
      "\n",
      "\n",
      "Here is the plan to follow as best as possible:\n",
      "\n",
      "- Clarify scope & assumptions (researcher): confirm we will (a) estimate training energy for a 24\u2011hour run on a single Azure Standard_NC6s_v3 VM, (b) estimate full pretraining cost where relevant as a separate \"full-run\" comparison, and (c) estimate per\u2011inference energy and dataset-level inference energy for standard datasets (ImageNet, GLUE, WebText). Choose default values and ranges for: VM/GPU power draw, PUE, and grid carbon intensity (and note region as a sensitivity parameter).\n",
      "\n",
      "- Collect authoritative inputs (researcher):\n",
      "  - Azure Standard_NC6s_v3 hardware/specs (GPU model, typical power draw) from Azure docs and NVIDIA V100 specs.\n",
      "  - Model facts: parameter counts, representative FLOPs per inference, typical training recipes/time-to-convergence from papers and reproducibility reports (ResNet\u201150/ImageNet, BERT\u2011base/GLUE, GPT\u20112/WebText).\n",
      "  - Dataset sizes / typical inference counts: ImageNet val/test size, GLUE typical dev/test sizes, WebText token counts or representative generation workloads.\n",
      "  - Typical data\u2011center PUE and regional carbon intensities (electricityMap, Azure Emissions Impact Dashboard) for sensitivity bounds.\n",
      "\n",
      "- Define calculation methods & assumptions (researcher + coder):\n",
      "  - Training energy for 24 h = (measured or estimated system power draw under load in kW) \u00d7 24 h \u00d7 PUE.\n",
      "  - CO2 = energy (kWh) \u00d7 carbon intensity (kg CO2/kWh).\n",
      "  - Per\u2011inference energy = (GPU+host incremental power under inference \u00d7 inference latency) \u00d7 PUE; or alternate FLOP-based estimate using J/FLOP constants for cross-check.\n",
      "  - For full pretraining estimates (optional), use published GPU\u2011hours or FLOPs \u00d7 energy/FLOP to estimate total energy and CO2.\n",
      "\n",
      "- Compute estimates & produce tables (coder):\n",
      "  - Implement calculations with chosen defaults and sensitivity ranges (low/typical/high) and produce clear tables:\n",
      "    - Table A: Training (24\u2011hour NC6s_v3) \u2014 model, estimated energy (kWh), CO2 (kg), notes on what fraction of a full training run this represents.\n",
      "    - Table B: Per\u2011inference energy and latency estimates \u2014 model, FLOPs, per\u2011inference energy (J and kWh), and example dataset total inference energy.\n",
      "    - Table C (optional): Full pretraining energy & CO2 estimates (published values and scaled single\u2011GPU estimates).\n",
      "  - Include brief footnotes listing assumptions and sources.\n",
      "\n",
      "- Recommend energy\u2011efficient model per task (researcher + coder):\n",
      "  - Use computed per\u2011task energy-per-performance heuristics (e.g., energy per top\u20111 accuracy point for ImageNet, energy per GLUE score point for BERT, energy per token for text generation) and provide a short justification for each recommendation.\n",
      "  - Provide sensitivity commentary and note caveats (distributed training, mixed precision, batch sizes, and that pretraining vs fine\u2011tuning differ drastically).\n",
      "\n",
      "- Deliverables:\n",
      "  - Short report with the tables, assumptions, main findings, and recommendations; include a compact sensitivity analysis.\n",
      "\n",
      "--------------------------\n",
      "\n",
      "[ORCH:instruction]\n",
      "\n",
      "Please compute the numeric estimates and produce the tables and recommendations requested by the user. Use the following assumptions and steps exactly (state results with clear units, and include short notes listing assumptions):\n",
      "\n",
      "Assumptions (fixed defaults):\n",
      "- VM: Azure Standard_NC6s_v3 (one VM for 24 hours). Assume GPU = NVIDIA V100.\n",
      "- Power draw (under training/inference load): GPU = 300 W, host CPU + other system = 50 W. Combined = 350 W (0.35 kW).\n",
      "- Data center PUE = 1.20 (multiply IT energy by 1.2 to get site energy).\n",
      "- Carbon intensity (default) = 0.5 kg CO2e per kWh (report this as a configurable parameter and include results for a low (0.1) and high (0.7) CI as sensitivity).\n",
      "\n",
      "Common model and dataset constants (use these canonical numbers):\n",
      "- ResNet-50: parameters = 25M; inference GFLOPs per image = 4.0 GFLOPs. Typical dataset for inference: ImageNet validation set = 50,000 images.\n",
      "- BERT-base: parameters = 110M; inference GFLOPs per input (seq length 128) = 20 GFLOPs. Typical GLUE evaluation aggregated examples (approx) = 8,000 examples (use 8k as dataset-size for whole GLUE eval run). Note: for text classification assume single-forward pass per example.\n",
      "- GPT-2 (small / 117M): parameters = 117M; inference GFLOPs per generated token = 10 GFLOPs (autoregressive per-token cost). For dataset-level inference, compute example for generating 1,000,000 tokens (1M tokens) as a realistic workload. \n",
      "\n",
      "GPU throughput and inference latency model:\n",
      "- Use peak practical Tensor-FLOPS for V100 FP16 with TensorCores = 125 TFLOPS (125e12 FLOPs/s) as a proxy for achievable inference FLOP throughput (acknowledging this is optimistic; note the assumption). \n",
      "- Compute per-inference latency = (FLOPs per inference) / (125e12 FLOPs/s).\n",
      "- Compute per-inference energy (IT energy, J) = combined_power_W * latency_s. Convert to kWh (1 kWh = 3.6e6 J). Multiply by PUE to get site energy in kWh.\n",
      "- Compute CO2 = site_energy_kWh * carbon_intensity_kgCO2_per_kWh.\n",
      "\n",
      "Training energy (24-hour run on NC6s_v3):\n",
      "- Energy (IT) = combined_power_kW * 24 h. Site energy = IT_energy * PUE. CO2 = site_energy * carbon_intensity.\n",
      "- Report this single 24-hour training-run energy and CO2 for each model (note: the 24h training energy is identical across models because the hardware/time are fixed; include a note showing how that 24h compares to typical full-training compute for each model where possible \u2014 e.g., fraction of a standard full pretraining/fine-tuning run). For the fraction-of-full-training note, use the following rough full-training GPU-hours (document as estimates):\n",
      "  - ResNet-50 (to standard ImageNet convergence): commonly ~90 GPU-hours on a V100-class GPU (single-GPU runs vary; note this is a rough single-GPU estimate). Use this to show what fraction 24h represents.\n",
      "  - BERT-base pretraining (from scratch): ~9,000 GPU-hours (order-of-magnitude; cite as rough). BERT-base fine-tuning on GLUE: ~1\u20133 GPU-hours per task (use 3h to be conservative); for full GLUE fine-tuning across tasks use 9 GPU-hours.\n",
      "  - GPT-2 (117M) pretraining: estimate ~1,000\u201310,000 GPU-hours depending on reproduction; use 3,000 GPU-hours as a mid-range illustrative estimate. Note these are approximate; state as such.\n",
      "\n",
      "Outputs required (tables and values):\n",
      "1) Table: \"Training energy & CO2 for 24-hour run on Azure NC6s_v3\"\n",
      "   Columns: Model, 24h IT energy (kWh), 24h site energy kWh (after PUE), CO2 (kg CO2) using default CI=0.5 kg/kWh, Note about fraction of typical full-training run completed in 24h (use the rough GPU-hour values above).\n",
      "\n",
      "2) Table: \"Per-inference energy and dataset-level inference energy\"\n",
      "   Rows for ResNet-50 (per image), BERT-base (per example seq=128), GPT-2 (per token); columns: FLOPs per inference (GFLOPs), latency (ms), per-inference IT energy (J), per-inference site energy (kWh), per-inference CO2 (mg or kg), dataset-level workload (ImageNet 50k images; GLUE 8k examples; GPT-2 1,000,000 tokens) -> total site energy (kWh) and total CO2 (kg) for that workload.\n",
      "\n",
      "3) Short summary recommendations:\n",
      "   - For image classification: which model is most energy-efficient per inference and for training-to-convergence (based on energy per top-1 accuracy point if you can compute a rough energy/accuracy metric; otherwise justify using per-inference energy + typical accuracy of ResNet-50).\n",
      "   - For text classification: recommend BERT-base or alternatives, justify using per-inference energy for GLUE-style classification and typical GLUE performance.\n",
      "   - For text generation: recommend GPT-2 (117M) as the most energy-efficient among the three for generation tasks, but explain that autoregressive generation per token is more expensive than one-shot classification and that smaller encoder models are cheaper for classification.\n",
      "\n",
      "4) Sensitivity notes: include CO2 numbers for carbon intensities 0.1, 0.5, 0.7 kgCO2/kWh; and comment briefly how results change if GPU power draw is \u00b120% or PUE is 1.1 vs 1.4.\n",
      "\n",
      "5) Deliver the outputs in concise tables and a final short paragraph with the recommended most energy-efficient model per task type (image classification, text classification, text generation) with 1\u20132 sentence justification each.\n",
      "\n",
      "Please run the calculations, produce the three tables, list the assumptions at the top of your output, and then produce the recommendations. If any value is an estimate or assumption, mark it clearly as such in a footnote row for the tables.\n",
      "--------------------------\n",
      "\n",
      "\n",
      "[STREAM:coder]: Assumptions (explicit; all results use these unless otherwise noted)\n",
      "- VM: Azure Standard_NC6s_v3 (1 VM for 24 hours). GPU = NVIDIA V100.\n",
      "- Power draw under load: GPU = 300 W, host+other = 50 W \u2192 combined = 350 W (0.35 kW).\n",
      "- Data center PUE = 1.20 (site energy = IT energy \u00d7 1.2).\n",
      "- Carbon intensity (CI) configurable; reported values: low = 0.1, default = 0.5, high = 0.7 kg CO2e / kWh.\n",
      "- Inference FLOPs and dataset sizes (canonical):\n",
      "  - ResNet-50: 4.0 GFLOPs / image; ImageNet validation = 50,000 images.\n",
      "  - BERT-base: 20 GFLOPs / example (seq=128); GLUE aggregated eval \u2248 8,000 examples (use 8k).\n",
      "  - GPT-2 (117M): 10 GFLOPs / generated token; example workload = 1,000,000 generated tokens.\n",
      "- Inference throughput model (optimistic proxy): V100 FP16 TensorCores = 125 TFLOPS = 125e12 FLOPs/s.\n",
      "  - Per-inference latency = FLOPs / 125e12 (s).\n",
      "  - Per-inference IT energy (J) = combined_power_W \u00d7 latency_s.\n",
      "  - Convert J \u2192 kWh by dividing by 3.6e6. Site energy = IT_kWh \u00d7 PUE.\n",
      "  - CO2 = site_energy_kWh \u00d7 CI (kg CO2).\n",
      "- Training 24-hour run (single NC6s_v3 VM, 24 h): IT energy = 0.35 kW \u00d7 24 h = 8.4 kWh; site energy = 8.4 \u00d7 1.2 = 10.08 kWh.\n",
      "- Rough full-training GPU-hour estimates (order-of-magnitude; marked as estimates):\n",
      "  - ResNet-50 to ImageNet convergence: ~90 GPU-hours (single-GPU rough).\n",
      "  - BERT-base pretraining (from scratch): ~9,000 GPU-hours (rough).\n",
      "  - BERT-base GLUE full fine-tuning (aggregate across tasks): use 9 GPU-hours (estimate: 3h/task \u00d7 3 tasks as conservative illustrative).\n",
      "  - GPT-2 (117M) pretraining: ~3,000 GPU-hours (mid-range illustrative).\n",
      "- Notes: The 125 TFLOPS throughput assumption is optimistic for real-world inference (peak FP16 TensorCore figure used as a proxy). All full-training GPU-hour numbers are rough estimates and are explicitly labeled as such.\n",
      "\n",
      "1) Table: \"Training energy & CO2 for 24-hour run on Azure NC6s_v3\"\n",
      "- Columns: Model, 24h IT energy (kWh), 24h site energy (kWh) (after PUE), CO2 (kg) using default CI = 0.5 kg/kWh, Note about fraction of typical full-training run completed in 24h.\n",
      "\n",
      "Training energy & CO2 for a single 24-hour run on Azure NC6s_v3 (V100) \u2014 CI default = 0.5 kg/kWh\n",
      "Model | 24h IT energy (kWh) | 24h site energy (kWh) | CO2 (kg) @ CI=0.5 | Note on fraction of typical full-training run\n",
      "--- | ---: | ---: | ---: | ---\n",
      "ResNet-50 | 8.4000 kWh | 10.0800 kWh | 5.0400 kg CO2 | 24h is 0.267 of typical ResNet-50 training (~90 GPU-h). (estimate)\n",
      "BERT-base | 8.4000 kWh | 10.0800 kWh | 5.0400 kg CO2 | 24h is 0.002667 of BERT-base pretraining (~9000 GPU-h); 24h is 2.667 of GLUE fine-tuning (~9 GPU-h). (estimates)\n",
      "GPT-2 (117M) | 8.4000 kWh | 10.0800 kWh | 5.0400 kg CO2 | 24h is 0.008000 of GPT-2 pretraining (~3000 GPU-h). (estimate)\n",
      "\n",
      "Footnote: full-training GPU-hour numbers are estimates (order-of-magnitude) and are shown to illustrate the fraction a 24h single-VM run represents.\n",
      "\n",
      "(If you extrapolate to estimate full-training energy from those GPU-hour estimates, example illustrative numbers \u2014 all approximate):\n",
      "- ResNet-50 full training (~90 GPU-h) \u2192 IT energy \u2248 0.35 kW \u00d7 90 h = 31.5 kWh; site \u2248 31.5 \u00d7 1.2 = 37.8 kWh; CO2 \u2248 37.8 \u00d7 0.5 = 18.9 kg CO2.\n",
      "- BERT-base pretraining (~9000 GPU-h) \u2192 site \u2248 (0.35 \u00d7 9000 \u00d7 1.2) = 3780 kWh; CO2 \u2248 3780 \u00d7 0.5 = 1,890 kg CO2. (estimate)\n",
      "- GPT-2 pretraining (~3000 GPU-h) \u2192 site \u2248 1260 kWh; CO2 \u2248 630 kg CO2. (estimate)\n",
      "\n",
      "2) Table: \"Per-inference energy and dataset-level inference energy\"\n",
      "- Columns requested: FLOPs per inference (GFLOPs), latency (ms), per-inference IT energy (J), per-inference site energy (kWh), per-inference CO2 (mg) or kg, dataset-level site energy (kWh) and total CO2 (kg) for the workload.\n",
      "\n",
      "Per-inference and dataset-level inference energy & CO2 (PUE=1.2, CI = 0.5 kg/kWh)\n",
      "Model | FLOPs per inference (GFLOPs) | Latency (ms) | Per-inference IT energy (J) | Per-inference site energy (kWh) | Per-inference CO2 (mg) @ CI=0.5 | Dataset size (examples/tokens) | Total site energy for dataset (kWh) | Total CO2 for dataset (kg) @ CI=0.5\n",
      "--- | ---: | ---: | ---: | ---: | ---: | ---: | ---: | ---:\n",
      "ResNet-50 | 4.00 GFLOPs | 0.032000 ms | 0.011200 J | 0.000000003733 kWh | 0.001867 mg | 50,000 images | 0.00018667 kWh | 0.00009333 kg\n",
      "BERT-base | 20.00 GFLOPs | 0.160000 ms | 0.056000 J | 0.000000018667 kWh | 0.009333 mg | 8,000 examples | 0.00014933 kWh | 0.00007467 kg\n",
      "GPT-2 (117M) | 10.00 GFLOPs (per token) | 0.080000 ms | 0.028000 J | 0.000000009333 kWh | 0.004667 mg | 1,000,000 tokens | 0.00933333 kWh | 0.00466667 kg\n",
      "\n",
      "Notes on table values and units:\n",
      "- Latency (ms) computed as (FLOPs / 125e12) \u00d7 1000.\n",
      "- Per-inference IT energy (J) = 350 W \u00d7 latency (s).\n",
      "- Per-inference site energy (kWh) = (IT energy J / 3.6e6) \u00d7 PUE.\n",
      "- Per-inference CO2 (mg) is site_energy_kWh \u00d7 0.5 kg/kWh \u00d7 1e6 mg/kg.\n",
      "- Dataset-level totals are simply per-inference site energy \u00d7 dataset size and corresponding CO2.\n",
      "\n",
      "Interpretation: per-inference energies are extremely small at the assumed peak throughput (the 125 TFLOPS number is an optimistic throughput proxy \u2014 real-world achievable throughput may be lower, increasing latency and energy). Even so, bulk workloads (e.g., 1,000,000 tokens) become measurable at dataset scale (GPT-2 example yields \u22480.00933 kWh site energy and \u22480.00467 kg CO2 at CI=0.5, still very small).\n",
      "\n",
      "3) Short summary recommendations (per task)\n",
      "\n",
      "- Image classification:\n",
      "  - Most energy-efficient per inference: ResNet-50 (4 GFLOPs \u2192 lowest per-inference FLOPs among listed models \u2192 lowest per-inference energy).\n",
      "  - Training-to-convergence: ResNet-50 is relatively cheap to fully train compared to large NLP pretraining (estimated full-training site energy ~37.8 kWh at ~90 GPU-h; CO2 ~18.9 kg at CI=0.5). If you need ImageNet-level accuracy (~ResNet-50 top-1 \u2248 76% \u2014 approximate), training energy per percentage-point (illustrative): ~37.8 kWh / 76 \u2248 0.50 kWh per top-1 % (estimate). Conclusion: for image classification, ResNet-50 is recommended when moderate accuracy and low inference cost are priorities.\n",
      "\n",
      "- Text classification (GLUE-style):\n",
      "  - Recommendation: BERT-base is a practical choice for GLUE-style classification; while per-inference FLOPs (20 GFLOPs) are higher than ResNet-50 per image, per-example energy remains extremely small. Importantly, full GLUE fine-tuning is inexpensive: estimated aggregate fine-tune (~9 GPU-h) has site energy \u2248 3.78 kWh and CO2 \u2248 1.89 kg (CI=0.5) \u2014 small in most operational contexts. If you only need classification (not generative text), encoder models (BERT variants or smaller distilled models) are more energy-efficient than using autoregressive generators.\n",
      "\n",
      "- Text generation:\n",
      "  - Recommendation: GPT-2 (117M) is the most appropriate among the three for generation tasks; it\u2019s relatively small and therefore more energy-efficient than larger autoregressive models. But autoregressive generation is per-token and therefore energy accumulates linearly with tokens; generation workloads can become expensive when many tokens are generated. For token-heavy generation, prefer smaller/gated models or constrained generation to reduce tokens; if generation quality can be met by smaller models, that yields large savings.\n",
      "\n",
      "4) Sensitivity notes (how CO2 changes with CI, GPU power, and PUE)\n",
      "\n",
      "- CI sensitivity for the 24-hour run (site energy = 10.08 kWh):\n",
      "  - CI = 0.1 kg/kWh \u2192 CO2 = 10.08 \u00d7 0.1 = 1.008 kg CO2 (24h).\n",
      "  - CI = 0.5 kg/kWh (default) \u2192 CO2 = 10.08 \u00d7 0.5 = 5.04 kg CO2 (24h).\n",
      "  - CI = 0.7 kg/kWh \u2192 CO2 = 10.08 \u00d7 0.7 = 7.056 kg CO2 (24h).\n",
      "\n",
      "- GPU power draw \u00b120% (affects combined_power):\n",
      "  - If combined power = 350 W \u00d7 0.8 = 280 W \u2192 24h site energy \u2248 8.064 kWh \u2192 CO2@0.5 \u2248 4.032 kg.\n",
      "  - If combined power = 350 W (base) \u2192 24h site energy \u2248 10.080 kWh \u2192 CO2@0.5 \u2248 5.040 kg.\n",
      "  - If combined power = 350 W \u00d7 1.2 = 420 W \u2192 24h site energy \u2248 12.096 kWh \u2192 CO2@0.5 \u2248 6.048 kg.\n",
      "  - So \u00b120% in power changes CO2 roughly \u00b120% linearly.\n",
      "\n",
      "- PUE sensitivity (CI fixed at 0.5):\n",
      "  - PUE = 1.1 \u2192 site energy = 0.35 \u00d7 24 \u00d7 1.1 = 9.24 kWh \u2192 CO2 \u2248 4.62 kg.\n",
      "  - PUE = 1.2 \u2192 site energy = 10.08 kWh \u2192 CO2 \u2248 5.04 kg (baseline).\n",
      "  - PUE = 1.4 \u2192 site energy = 11.76 kWh \u2192 CO2 \u2248 5.88 kg.\n",
      "  - So PUE variation meaningfully changes site energy and CO2; reducing PUE is an effective mitigation.\n",
      "\n",
      "5) Final concise recommendations (one-liners with justification)\n",
      "\n",
      "- Image classification: ResNet-50 \u2014 lowest per-inference FLOPs (4 GFLOPs) among the listed models and modest training cost to converge (estimated full-train site energy ~37.8 kWh). Use ResNet-50 or a smaller/resnet-distilled model for best inference energy efficiency when you only need moderate accuracy.\n",
      "\n",
      "- Text classification (GLUE-style): BERT-base \u2014 appropriate compromise between accuracy and cost; per-inference energy is still very small at the assumed throughput, and GLUE fine-tuning has a low aggregate energy cost (estimated ~3.78 kWh site for full GLUE fine-tune). Consider distilled/smaller encoders if you need lower latency/energy.\n",
      "\n",
      "- Text generation: GPT-2 (117M) \u2014 best choice among these for generation (smaller autoregressive model), but autoregressive generation costs accumulate per token (10 GFLOPs/token here), so limit sequence length or use smaller generators for heavy-generation workloads.\n",
      "\n",
      "Everything above follows the requested computation steps and assumptions. Numbers that are estimates are explicitly marked (throughout the notes and footnotes) \u2014 specifically:\n",
      "- The 125 TFLOPS figure is an optimistic/proxy peak throughput for V100 FP16 TensorCores (used to compute latencies and hence energy per inference).\n",
      "- Full-training GPU-hour figures are rough order-of-magnitude estimates and are explicitly labeled \"estimate\".\n",
      "- Per-inference and dataset totals are computed precisely from the stated assumptions.\n",
      "\n",
      "If you\u2019d like:\n",
      "- I can re-run these tables with a different throughput assumption (e.g., 50 TFLOPS or a measured real-world throughput) to show a less optimistic latency/energy profile.\n",
      "- Or I can produce CSV/Excel downloads of the tables. (final)\n",
      "\n",
      "\n",
      "[AGENT:coder] assistant\n",
      "\n",
      "Assumptions (explicit; all results use these unless otherwise noted) - VM: Azure Standard_NC6s_v3 (1 VM for 24 hours). GPU = NVIDIA V100. - Power draw under load: GPU = 300 W, host+other = 50 W \u2192 combined = 350 W (0.35 kW). - Data center PUE = 1.20 (site energy = IT energy \u00d7 1.2). - Carbon intensity (CI) configurable; reported values: low = 0.1, default = 0.5, high = 0.7 kg CO2e / kWh. - Inference FLOPs and dataset sizes (canonical):   - ResNet-50: 4.0 GFLOPs / image; ImageNet validation = 50,000 images.   - BERT-base: 20 GFLOPs / example (seq=128); GLUE aggregated eval \u2248 8,000 examples (use 8k).   - GPT-2 (117M): 10 GFLOPs / generated token; example workload = 1,000,000 generated tokens. - Inference throughput model (optimistic proxy): V100 FP16 TensorCores = 125 TFLOPS = 125e12 FLOPs/s.   - Per-inference latency = FLOPs / 125e12 (s).   - Per-inference IT energy (J) = combined_power_W \u00d7 latency_s.   - Convert J \u2192 kWh by dividing by 3.6e6. Site energy = IT_kWh \u00d7 PUE.   - CO2 = site_energy_kWh \u00d7 CI (kg CO2). - Training 24-hour run (single NC6s_v3 VM, 24 h): IT energy = 0.35 kW \u00d7 24 h = 8.4 kWh; site energy = 8.4 \u00d7 1.2 = 10.08 kWh. - Rough full-training GPU-hour estimates (order-of-magnitude; marked as estimates):   - ResNet-50 to ImageNet convergence: ~90 GPU-hours (single-GPU rough).   - BERT-base pretraining (from scratch): ~9,000 GPU-hours (rough).   - BERT-base GLUE full fine-tuning (aggregate across tasks): use 9 GPU-hours (estimate: 3h/task \u00d7 3 tasks as conservative illustrative).   - GPT-2 (117M) pretraining: ~3,000 GPU-hours (mid-range illustrative). - Notes: The 125 TFLOPS throughput assumption is optimistic for real-world inference (peak FP16 TensorCore figure used as a proxy). All full-training GPU-hour numbers are rough estimates and are explicitly labeled as such.  1) Table: \"Training energy & CO2 for 24-hour run on Azure NC6s_v3\" - Columns: Model, 24h IT energy (kWh), 24h site energy (kWh) (after PUE), CO2 (kg) using default CI = 0.5 kg/kWh, Note about fraction of typical full-training run completed in 24h.  Training energy & CO2 for a single 24-hour run on Azure NC6s_v3 (V100) \u2014 CI default = 0.5 kg/kWh Model | 24h IT energy (kWh) | 24h site energy (kWh) | CO2 (kg) @ CI=0.5 | Note on fraction of typical full-training run --- | ---: | ---: | ---: | --- ResNet-50 | 8.4000 kWh | 10.0800 kWh | 5.0400 kg CO2 | 24h is 0.267 of typical ResNet-50 training (~90 GPU-h). (estimate) BERT-base | 8.4000 kWh | 10.0800 kWh | 5.0400 kg CO2 | 24h is 0.002667 of BERT-base pretraining (~9000 GPU-h); 24h is 2.667 of GLUE fine-tuning (~9 GPU-h). (estimates) GPT-2 (117M) | 8.4000 kWh | 10.0800 kWh | 5.0400 kg CO2 | 24h is 0.008000 of GPT-2 pretraining (~3000 GPU-h). (estimate)  Footnote: full-training GPU-hour numbers are estimates (order-of-magnitude) and are shown to illustrate the fraction a 24h single-VM run represents.  (If you extrapolate to estimate full-training energy from those GPU-hour estimates, example illustrative numbers \u2014 all approximate): - ResNet-50 full training (~90 GPU-h) \u2192 IT energy \u2248 0.35 kW \u00d7 90 h = 31.5 kWh; site \u2248 31.5 \u00d7 1.2 = 37.8 kWh; CO2 \u2248 37.8 \u00d7 0.5 = 18.9 kg CO2. - BERT-base pretraining (~9000 GPU-h) \u2192 site \u2248 (0.35 \u00d7 9000 \u00d7 1.2) = 3780 kWh; CO2 \u2248 3780 \u00d7 0.5 = 1,890 kg CO2. (estimate) - GPT-2 pretraining (~3000 GPU-h) \u2192 site \u2248 1260 kWh; CO2 \u2248 630 kg CO2. (estimate)  2) Table: \"Per-inference energy and dataset-level inference energy\" - Columns requested: FLOPs per inference (GFLOPs), latency (ms), per-inference IT energy (J), per-inference site energy (kWh), per-inference CO2 (mg) or kg, dataset-level site energy (kWh) and total CO2 (kg) for the workload.  Per-inference and dataset-level inference energy & CO2 (PUE=1.2, CI = 0.5 kg/kWh) Model | FLOPs per inference (GFLOPs) | Latency (ms) | Per-inference IT energy (J) | Per-inference site energy (kWh) | Per-inference CO2 (mg) @ CI=0.5 | Dataset size (examples/tokens) | Total site energy for dataset (kWh) | Total CO2 for dataset (kg) @ CI=0.5 --- | ---: | ---: | ---: | ---: | ---: | ---: | ---: | ---: ResNet-50 | 4.00 GFLOPs | 0.032000 ms | 0.011200 J | 0.000000003733 kWh | 0.001867 mg | 50,000 images | 0.00018667 kWh | 0.00009333 kg BERT-base | 20.00 GFLOPs | 0.160000 ms | 0.056000 J | 0.000000018667 kWh | 0.009333 mg | 8,000 examples | 0.00014933 kWh | 0.00007467 kg GPT-2 (117M) | 10.00 GFLOPs (per token) | 0.080000 ms | 0.028000 J | 0.000000009333 kWh | 0.004667 mg | 1,000,000 tokens | 0.00933333 kWh | 0.00466667 kg  Notes on table values and units: - Latency (ms) computed as (FLOPs / 125e12) \u00d7 1000. - Per-inference IT energy (J) = 350 W \u00d7 latency (s). - Per-inference site energy (kWh) = (IT energy J / 3.6e6) \u00d7 PUE. - Per-inference CO2 (mg) is site_energy_kWh \u00d7 0.5 kg/kWh \u00d7 1e6 mg/kg. - Dataset-level totals are simply per-inference site energy \u00d7 dataset size and corresponding CO2.  Interpretation: per-inference energies are extremely small at the assumed peak throughput (the 125 TFLOPS number is an optimistic throughput proxy \u2014 real-world achievable throughput may be lower, increasing latency and energy). Even so, bulk workloads (e.g., 1,000,000 tokens) become measurable at dataset scale (GPT-2 example yields \u22480.00933 kWh site energy and \u22480.00467 kg CO2 at CI=0.5, still very small).  3) Short summary recommendations (per task)  - Image classification:   - Most energy-efficient per inference: ResNet-50 (4 GFLOPs \u2192 lowest per-inference FLOPs among listed models \u2192 lowest per-inference energy).   - Training-to-convergence: ResNet-50 is relatively cheap to fully train compared to large NLP pretraining (estimated full-training site energy ~37.8 kWh at ~90 GPU-h; CO2 ~18.9 kg at CI=0.5). If you need ImageNet-level accuracy (~ResNet-50 top-1 \u2248 76% \u2014 approximate), training energy per percentage-point (illustrative): ~37.8 kWh / 76 \u2248 0.50 kWh per top-1 % (estimate). Conclusion: for image classification, ResNet-50 is recommended when moderate accuracy and low inference cost are priorities.  - Text classification (GLUE-style):   - Recommendation: BERT-base is a practical choice for GLUE-style classification; while per-inference FLOPs (20 GFLOPs) are higher than ResNet-50 per image, per-example energy remains extremely small. Importantly, full GLUE fine-tuning is inexpensive: estimated aggregate fine-tune (~9 GPU-h) has site energy \u2248 3.78 kWh and CO2 \u2248 1.89 kg (CI=0.5) \u2014 small in most operational contexts. If you only need classification (not generative text), encoder models (BERT variants or smaller distilled models) are more energy-efficient than using autoregressive generators.  - Text generation:   - Recommendation: GPT-2 (117M) is the most appropriate among the three for generation tasks; it\u2019s relatively small and therefore more energy-efficient than larger autoregressive models. But autoregressive generation is per-token and therefore energy accumulates linearly with tokens; generation workloads can become expensive when many tokens are generated. For token-heavy generation, prefer smaller/gated models or constrained generation to reduce tokens; if generation quality can be met by smaller models, that yields large savings.  4) Sensitivity notes (how CO2 changes with CI, GPU power, and PUE)  - CI sensitivity for the 24-hour run (site energy = 10.08 kWh):   - CI = 0.1 kg/kWh \u2192 CO2 = 10.08 \u00d7 0.1 = 1.008 kg CO2 (24h).   - CI = 0.5 kg/kWh (default) \u2192 CO2 = 10.08 \u00d7 0.5 = 5.04 kg CO2 (24h).   - CI = 0.7 kg/kWh \u2192 CO2 = 10.08 \u00d7 0.7 = 7.056 kg CO2 (24h).  - GPU power draw \u00b120% (affects combined_power):   - If combined power = 350 W \u00d7 0.8 = 280 W \u2192 24h site energy \u2248 8.064 kWh \u2192 CO2@0.5 \u2248 4.032 kg.   - If combined power = 350 W (base) \u2192 24h site energy \u2248 10.080 kWh \u2192 CO2@0.5 \u2248 5.040 kg.   - If combined power = 350 W \u00d7 1.2 = 420 W \u2192 24h site energy \u2248 12.096 kWh \u2192 CO2@0.5 \u2248 6.048 kg.   - So \u00b120% in power changes CO2 roughly \u00b120% linearly.  - PUE sensitivity (CI fixed at 0.5):   - PUE = 1.1 \u2192 site energy = 0.35 \u00d7 24 \u00d7 1.1 = 9.24 kWh \u2192 CO2 \u2248 4.62 kg.   - PUE = 1.2 \u2192 site energy = 10.08 kWh \u2192 CO2 \u2248 5.04 kg (baseline).   - PUE = 1.4 \u2192 site energy = 11.76 kWh \u2192 CO2 \u2248 5.88 kg.   - So PUE variation meaningfully changes site energy and CO2; reducing PUE is an effective mitigation.  5) Final concise recommendations (one-liners with justification)  - Image classification: ResNet-50 \u2014 lowest per-inference FLOPs (4 GFLOPs) among the listed models and modest training cost to converge (estimated full-train site energy ~37.8 kWh). Use ResNet-50 or a smaller/resnet-distilled model for best inference energy efficiency when you only need moderate accuracy.  - Text classification (GLUE-style): BERT-base \u2014 appropriate compromise between accuracy and cost; per-inference energy is still very small at the assumed throughput, and GLUE fine-tuning has a low aggregate energy cost (estimated ~3.78 kWh site for full GLUE fine-tune). Consider distilled/smaller encoders if you need lower latency/energy.  - Text generation: GPT-2 (117M) \u2014 best choice among these for generation (smaller autoregressive model), but autoregressive generation costs accumulate per token (10 GFLOPs/token here), so limit sequence length or use smaller generators for heavy-generation workloads.  Everything above follows the requested computation steps and assumptions. Numbers that are estimates are explicitly marked (throughout the notes and footnotes) \u2014 specifically: - The 125 TFLOPS figure is an optimistic/proxy peak throughput for V100 FP16 TensorCores (used to compute latencies and hence energy per inference). - Full-training GPU-hour figures are rough order-of-magnitude estimates and are explicitly labeled \"estimate\". - Per-inference and dataset totals are computed precisely from the stated assumptions.  If you\u2019d like: - I can re-run these tables with a different throughput assumption (e.g., 50 TFLOPS or a measured real-world throughput) to show a less optimistic latency/energy profile. - Or I can produce CSV/Excel downloads of the tables.\n",
      "--------------------------\n",
      "\n",
      "==================================================\n",
      "FINAL RESULT:\n",
      "==================================================\n",
      "Below are concise, reproducible estimates (with the exact assumptions you requested) comparing training and inference energy and CO2 for ResNet\u201150, BERT\u2011base, and GPT\u20112 (117M). Values that are approximate/estimated are explicitly marked.\n",
      "\n",
      "Key assumptions (all results use these unless otherwise noted)\n",
      "- VM: Azure Standard_NC6s_v3 (1 VM, 24 hours). GPU = NVIDIA V100.\n",
      "- Power draw under load: GPU 300 W + host/other 50 W \u2192 combined = 350 W (0.35 kW). (assumption)\n",
      "- Data\u2011center PUE = 1.20. (assumption)\n",
      "- Carbon intensity (CI) scenarios: low = 0.1, default = 0.5, high = 0.7 kg CO2/kWh. (configurable)\n",
      "- Inference FLOPs & dataset sizes (canonical):\n",
      "  - ResNet\u201150: 4.0 GFLOPs/image; ImageNet val = 50,000 images.\n",
      "  - BERT\u2011base: 20 GFLOPs/example (seq len 128); GLUE aggregated eval \u2248 8,000 examples. (approx.)\n",
      "  - GPT\u20112 (117M): 10 GFLOPs/generated token; example workload = 1,000,000 tokens. (assumption)\n",
      "- Inference throughput proxy (optimistic): V100 FP16 TensorCore peak = 125 TFLOPS = 125\u00d710^12 FLOPs/s. (optimistic proxy; real throughput lower)  \n",
      "  - latency = FLOPs / 125e12 (s)\n",
      "  - per\u2011inference IT energy (J) = combined_power_W \u00d7 latency_s\n",
      "  - convert J \u2192 kWh: / 3.6e6; site energy = IT_kWh \u00d7 PUE\n",
      "  - CO2 = site_kWh \u00d7 CI\n",
      "- Training 24\u2011hour energy (single VM): IT energy = 0.35 kW \u00d7 24 h = 8.4 kWh; site energy = 8.4 \u00d7 1.2 = 10.08 kWh.\n",
      "- Representative full\u2011training GPU\u2011hour estimates (order\u2011of\u2011magnitude, marked \u201cestimate\u201d):\n",
      "  - ResNet\u201150 (ImageNet converge) \u2248 90 GPU\u2011hours (estimate).\n",
      "  - BERT\u2011base pretraining \u2248 9,000 GPU\u2011hours (estimate); GLUE fine\u2011tuning aggregate \u2248 9 GPU\u2011hours (estimate).\n",
      "  - GPT\u20112 (117M) pretraining \u2248 3,000 GPU\u2011hours (estimate).\n",
      "\n",
      "Table 1 \u2014 Training energy & CO2 for a single 24\u2011hour run on Azure NC6s_v3 (V100)\n",
      "(all models use the same single\u2011VM 24 h run; CI default = 0.5 kgCO2/kWh)\n",
      "\n",
      "Model | 24 h IT energy (kWh) | 24 h site energy (kWh, after PUE) | CO2 (kg) @ CI=0.5 | Note: fraction of typical full\u2011training (estimate)\n",
      "--- | ---: | ---: | ---: | ---\n",
      "ResNet\u201150 | 8.40 | 10.08 | 5.04 kg CO2 | 24 h = 24/90 \u2248 0.267 of a ~90 GPU\u2011h full run (estimate)\n",
      "BERT\u2011base | 8.40 | 10.08 | 5.04 kg CO2 | 24 h = 24/9000 \u2248 0.0027 of full pretraining (~9,000 GPU\u2011h); 24 h \u2248 24/9 \u2248 2.7 \u00d7 full GLUE fine\u2011tune (estimate)\n",
      "GPT\u20112 (117M) | 8.40 | 10.08 | 5.04 kg CO2 | 24 h = 24/3000 \u2248 0.008 of full pretraining (~3,000 GPU\u2011h) (estimate)\n",
      "\n",
      "\u2014 Notes:\n",
      "- Training energy and CO2 for a 24 h single\u2011VM run are identical across models because the hardware/time are fixed.\n",
      "- Full\u2011training GPU\u2011hour numbers are rough, illustrative estimates and vary widely with batch size, distributed setup, and hyperparameters.\n",
      "\n",
      "Table 2 \u2014 Per\u2011inference energy, latency, and dataset\u2011level totals (PUE=1.2, CI default = 0.5)\n",
      "(Using the 125 TFLOPS optimistic throughput proxy; mark as optimistic. Per\u2011inference CO2 shown in micrograms and mg for readability.)\n",
      "\n",
      "Model | FLOPs per inference | Latency (ms) | Per\u2011inference IT energy (J) | Per\u2011inference site energy (kWh) | Per\u2011inf CO2 (@0.5 CI) | Dataset workload | Total site energy (kWh) | Total CO2 (kg) @0.5 CI\n",
      "--- | ---: | ---: | ---: | ---: | ---: | ---: | ---: | ---\n",
      "ResNet\u201150 | 4.0 GFLOPs | 0.032 ms | 0.0112 J | 3.733\u00d710^-9 kWh | 1.87\u00d710^-9 kg = 0.00187 mg | ImageNet val (50,000 imgs) | 0.0001867 kWh | 0.0000933 kg\n",
      "BERT\u2011base | 20 GFLOPs | 0.160 ms | 0.0560 J | 1.8667\u00d710^-8 kWh | 9.33\u00d710^-9 kg = 0.00933 mg | GLUE eval (8,000 ex) | 0.0001493 kWh | 0.0000747 kg\n",
      "GPT\u20112 (117M) | 10 GFLOPs/token | 0.080 ms | 0.0280 J | 9.333\u00d710^-9 kWh | 4.67\u00d710^-9 kg = 0.00467 mg | 1,000,000 tokens | 0.0093333 kWh | 0.0046667 kg\n",
      "\n",
      "\u2014 Notes:\n",
      "- Latency = FLOPs / 125e12 \u00d7 1000 ms.\n",
      "- IT energy (J) = 350 W \u00d7 latency (s). Site energy multiplies IT kWh by PUE (1.2).\n",
      "- Dataset totals = per\u2011inf site kWh \u00d7 workload size.\n",
      "- Per\u2011inf CO2 are extremely small under optimistic throughput; real-world latencies/energy will be larger if actual throughput <125 TFLOPS.\n",
      "\n",
      "Optional illustrative extrapolation: approximate full\u2011training energy and CO2 (using the rough GPU\u2011hour estimates)\n",
      "- ResNet\u201150 full training (~90 GPU\u2011h): site energy \u2248 0.35 kW \u00d7 90 h \u00d7 1.2 = 37.8 kWh \u2192 CO2@0.5 \u2248 18.9 kg (estimate)\n",
      "- BERT\u2011base pretraining (~9000 GPU\u2011h): site \u2248 0.35 \u00d7 9000 \u00d7 1.2 = 3,780 kWh \u2192 CO2@0.5 \u2248 1,890 kg (estimate)\n",
      "- GPT\u20112 (117M) pretraining (~3000 GPU\u2011h): site \u2248 0.35 \u00d7 3000 \u00d7 1.2 = 1,260 kWh \u2192 CO2@0.5 \u2248 630 kg (estimate)\n",
      "(These full\u2011training numbers are coarse, order\u2011of\u2011magnitude estimates \u2014 use with caution.)\n",
      "\n",
      "Sensitivity snapshots (24\u2011hour single VM site energy = 10.08 kWh baseline)\n",
      "- Carbon intensity (24 h run CO2):\n",
      "  - CI = 0.1 \u2192 CO2 = 10.08 \u00d7 0.1 = 1.008 kg\n",
      "  - CI = 0.5 \u2192 CO2 = 10.08 \u00d7 0.5 = 5.040 kg (baseline)\n",
      "  - CI = 0.7 \u2192 CO2 = 10.08 \u00d7 0.7 = 7.056 kg\n",
      "- GPU combined power \u00b120% (affects 24 h energy linearly):\n",
      "  - \u221220% power (280 W combined) \u2192 site energy = (0.28\u00d724)\u00d71.2 = 8.064 kWh \u2192 CO2@0.5 = 4.032 kg\n",
      "  - +20% power (420 W) \u2192 site energy = (0.42\u00d724)\u00d71.2 = 12.096 kWh \u2192 CO2@0.5 = 6.048 kg\n",
      "- PUE effect (combined_power=350 W):\n",
      "  - PUE=1.1 \u2192 site energy = 9.24 kWh \u2192 CO2@0.5 = 4.62 kg\n",
      "  - PUE=1.4 \u2192 site energy = 11.76 kWh \u2192 CO2@0.5 = 5.88 kg\n",
      "\n",
      "Recommendations (concise, with justification)\n",
      "\n",
      "- Image classification \u2014 Most energy\u2011efficient: ResNet\u201150 (among these three)\n",
      "  - Why: lowest inference FLOPs (4 GFLOPs/image) \u2192 lowest per\u2011inference energy. Full training to ImageNet convergence is modest vs large NLP pretraining (~90 GPU\u2011h estimate), so overall training+inference energy for standard ImageNet tasks is lower. If you prioritize even lower energy/latency, use smaller architectures or distilled/resNet variants.\n",
      "\n",
      "- Text classification (GLUE\u2011style) \u2014 Most energy\u2011efficient: BERT\u2011base (practical choice), with caveat to favor smaller distilled encoders if available\n",
      "  - Why: BERT\u2011base offers strong task accuracy per moderate compute; GLUE fine\u2011tuning is cheap in aggregate (few GPU\u2011hours). For deployment, consider DistilBERT or other distilled encoders for lower inference cost if accuracy tradeoffs are acceptable.\n",
      "\n",
      "- Text generation \u2014 Most energy\u2011efficient: GPT\u20112 (117M) among these three for generation tasks\n",
      "  - Why: It\u2019s the only autoregressive generator in the set and is the smallest autoregressive model here; however generation cost accumulates per token (10 GFLOPs/token assumed), so generation-heavy workloads can become costly. For heavy generation consider smaller generators or restricting token output.\n",
      "\n",
      "Caveats & important notes (short)\n",
      "- The single\u2011VM 24\u2011hour training energy (10.08 kWh site, 5.04 kgCO2 @CI=0.5) is identical across models because the hardware/time are fixed; the meaningful model\u2011level differences are in how much of a full training run 24 h represents and in per\u2011inference cost.\n",
      "- The 125 TFLOPS figure is an optimistic peak FP16 TensorCore number for a V100 and therefore yields very small per\u2011inference latencies/energies; real application throughput will typically be lower (increasing latency and energy per inference). If you want, I can re\u2011compute the per\u2011inference table with a more conservative throughput (e.g., 20\u201350 TFLOPS) or using measured latencies.\n",
      "- Full\u2011training GPU\u2011hour numbers (ResNet 90 h, BERT 9,000 h, GPT\u20112 3,000 h) are rough, order\u2011of\u2011magnitude estimates \u2014 actual runs depend heavily on batch size, optimization, distributed training, mixed precision, and training recipes.\n",
      "\n",
      "If you\u2019d like next steps, I can:\n",
      "- Recompute the per\u2011inference numbers using a lower throughput (e.g., 50 TFLOPS or measured latencies), which will give more conservative inference energy estimates.\n",
      "- Produce CSV/Excel exports of the tables or a short slide summarizing findings.\n",
      "- Recompute full\u2011training energy with alternate full\u2011training GPU\u2011hour estimates or real published GPU\u2011hour values for specific reproducible recipes.\n",
      "==================================================\n",
      "\n",
      "\n",
      "Workflow completed with result:\n",
      "\n",
      "<agent_framework._types.ChatMessage object at 0x11488e240>\n"
     ]
    }
   ],
   "source": [
    "def reset_observers() -> None:\n",
    "    notebook_ui.reset()\n",
    "    approval_handler.reset()\n",
    "\n",
    "\n",
    "async def run_magentic_demo(task: str, *, resume_from: str | None = None) -> str:\n",
    "    reset_observers()\n",
    "    fleet.set_workflow_id(\"notebook-demo\")\n",
    "    result = await fleet.run(task, resume_from_checkpoint=resume_from)\n",
    "    return result\n",
    "\n",
    "\n",
    "def build_summary() -> dict[str, Any]:\n",
    "    summary = notebook_ui.latest_summary()\n",
    "    summary[\"approvals\"] = [\n",
    "        {\n",
    "            \"request_id\": response.request_id,\n",
    "            \"decision\": response.decision.value,\n",
    "            \"reason\": response.reason,\n",
    "        }\n",
    "        for response in approval_handler.history\n",
    "    ]\n",
    "    return summary\n",
    "\n",
    "\n",
    "def display_summary() -> None:\n",
    "    if not notebook_ui.plan_history:\n",
    "        print(\"No orchestration run has been recorded yet.\")\n",
    "        return\n",
    "    print(json.dumps(build_summary(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e2bae3",
   "metadata": {},
   "source": [
    "## Execute the workflow (optional)\n",
    "\n",
    "Run the coroutine in an async-enabled notebook cell (IPython already exposes `await`). The run streams Magentic events, applies the notebook HITL policy, and persists checkpoints when enabled.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8b69f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the lines below after configuring credentials to execute the workflow.\n",
    "# result_text = await run_magentic_demo(TASK_REQUEST)\n",
    "# print(result_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89601071",
   "metadata": {},
   "source": [
    "### Inspect orchestration artifacts\n",
    "\n",
    "Use the helper below after a run to show every plan, progress ledger evaluation, agent response, and approval captured during orchestration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80b7ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7dbdbcd",
   "metadata": {},
   "source": [
    "### Plan review details\n",
    "\n",
    "The approval handler stores every plan review interaction. The snippet below highlights how the first plan is revised while subsequent requests are auto-approved.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f9abbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "    {\n",
    "        \"request_id\": response.request_id,\n",
    "        \"decision\": response.decision.value,\n",
    "        \"reason\": response.reason,\n",
    "        \"modified_plan_preview\": (response.modified_code or \"\")[:200],\n",
    "    }\n",
    "    for response in approval_handler.history\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a492601b",
   "metadata": {},
   "source": [
    "### Checkpoint catalog\n",
    "\n",
    "When checkpointing is enabled, this call returns the available resume points for the current workflow identifier (`notebook-demo`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a64be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# await fleet.list_checkpoints()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40eff381",
   "metadata": {},
   "source": [
    "### Resume from a checkpoint (optional)\n",
    "\n",
    "Pick a checkpoint identifier from the catalog and pass it to `run_magentic_demo`. This lets you inspect the behavior of the orchestrator when recovering mid-plan.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76659fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint_id = \"fleet_xxxxxxxx\"  # Replace with a value from fleet.list_checkpoints()\n",
    "# result_from_resume = await run_magentic_demo(TASK_REQUEST, resume_from=checkpoint_id)\n",
    "# print(result_from_resume)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc66843d",
   "metadata": {},
   "source": [
    "### Next steps\n",
    "\n",
    "- Swap in different tasks to stress-test delegation and replanning.\n",
    "- Toggle HITL requirements in `workflow.yaml` to compare automated versus interactive plan reviews.\n",
    "- Use the checkpoint catalog to resume stalled runs or branch off alternative investigation paths.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentic-fleet (3.12.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
