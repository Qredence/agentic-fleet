# DSPy Configuration
dspy:
  model: gpt-5-mini # High-capacity model for DSPy tasks
  routing_model: gpt-5-mini # Fast model for analysis/routing phases (Plan #4)
  temperature: 1.0
  max_tokens: 16000
  compiled_reasoner_path: .var/cache/dspy/compiled_reasoner.json
  # Phase 1: Compiled artifact paths for decision modules
  compiled_routing_path: .var/cache/dspy/compiled_routing.json
  compiled_tool_planning_path: .var/cache/dspy/compiled_tool_planning.json
  compiled_quality_path: .var/logs/compiled_answer_quality.pkl
  # When true, raise an error if no compiled DSPy artifact is found.
  # Recommended for production environments to prevent degraded performance
  # from zero-shot fallback. Run 'agentic-fleet optimize' before enabling.
  require_compiled: false
  # DSPy 3.x TypedPredictor settings
  use_typed_signatures: true # Enable Pydantic-based typed signatures for structured outputs
  enable_routing_cache: true # Cache routing decisions to avoid redundant LLM calls
  routing_cache_ttl_seconds: 300 # TTL for routing cache entries (5 minutes)

  # Dynamic Prompt Signatures
  # Agent instructions can be generated dynamically using DSPy signatures defined in
  # src/agentic_fleet/dspy_modules/signatures.py, enabling context-aware, optimizer-tunable
  # prompts instead of static templates.
  #
  # Currently Implemented:
  # - PlannerInstructionSignature: Generates specialized instructions for the Planner/Orchestrator
  #   agent based on available agents and workflow goals. Used when an agent has
  #   instructions: prompts.planner in the agents: section below.
  #
  # How it works:
  # 1. workflow_config.yaml defines agents with instructions: prompts.{agent_name}
  # 2. AgentFactory (src/agentic_fleet/agents/coordinator.py) reads the config
  # 3. _resolve_instructions() checks if dynamic prompt generation is available
  # 4. For planner: Uses dspy.ChainOfThought(PlannerInstructionSignature) to generate instructions
  # 5. For other agents: Falls back to static prompts from agents/prompts.py
  #
  # Benefits: Offline optimization via DSPy's GEPA optimizer makes prompts tunable and
  # context-aware rather than static. The goal is to migrate all agents to use DSPy signatures.
  #
  # Future: AgentInstructionSignature exists for expansion to other agents (executor, coder, etc.)
  # See: src/agentic_fleet/agents/coordinator.py (AgentFactory._resolve_instructions)
  dynamic_prompts:
    enabled: true # Enable dynamic prompt generation (requires enable_dspy_agents: true)
    signatures_path: src/agentic_fleet/dspy_modules/signatures.py
    # Current implementation: Only planner uses dynamic prompts via PlannerInstructionSignature
    # Other agents (executor, coder, verifier, etc.) use static prompts from prompts.py

  # Azure AI Foundry Configuration
  foundry:
    enabled: true
    project_connection_string: ${AZURE_AI_PROJECT_ENDPOINT}
    # Optional: Semantic narrator model name in Foundry
    narrator_model: gpt-5.1-nano

  optimization:
    enabled: true
    examples_path: src/agentic_fleet/data/supervisor_examples.json
    metric_threshold: 0.8
    max_bootstrapped_demos: 4
    use_gepa: true
    # GEPA exclusivity: choose EXACTLY ONE of:
    #   1) gepa_auto: light|medium|heavy   (preset internal iteration budgets)
    #   2) gepa_max_full_evals: <int>      (hard cap on full evaluation cycles)
    #   3) gepa_max_metric_calls: <int>    (hard cap on metric scoring calls)
    # If more than one is set, the console/runner will resolve priority: auto > max_full_evals > max_metric_calls.
    gepa_auto: light # Using light preset for balanced optimization speed
    # gepa_max_full_evals: 3  # Disabled - using gepa_auto instead
    # gepa_max_metric_calls: 3 # Disabled - using gepa_auto instead
    gepa_reflection_model: gpt-5-mini # Use fast model for reflection to reduce latency
    gepa_reflection_minibatch_size: 2 # Smaller batches for faster reflection cycles
    gepa_log_dir: .var/logs/dspy/gepa
    gepa_perfect_score: 1.0
    gepa_use_history_examples: true
    gepa_history_min_quality: 8.0
    gepa_history_limit: 200
    gepa_val_split: 0.2
    gepa_seed: 13
    # Optimizer fallback chain: gepa -> bootstrap -> zero-shot
    fallback_to_bootstrap: true # Fall back to BootstrapFewShot if GEPA fails
    validate_examples: true # Validate training examples before optimization

# API Configuration
api:
  chat:
    # Agent IDs to include in streaming responses (only these agents' messages will be shown)
    # By default, only show messages from orchestration agents that synthesize the final response
    included_agent_ids:
      - orchestrator
      - researcher
      - analyst
      - writer
      - reviewer
      - planner
      - executor
      - coder
      - verifier

# Workflow Configuration
workflow:
  supervisor:
    max_rounds: 15
    max_stalls: 3
    max_resets: 2
    enable_streaming: true
    max_task_length: 10000
    dspy_retry_attempts: 3
    dspy_retry_backoff_seconds: 1.0
    analysis_cache_ttl_seconds: 3600
    # Inject recent conversation messages into analysis/routing so short
    # follow-up replies (e.g., quick-reply buttons) stay contextual.
    conversation_context_max_messages: 8
    conversation_context_max_chars: 4000

  execution:
    parallel_threshold: 3
    timeout_seconds: 120 # Reduced from 300 to 120 seconds to prevent hanging
    retry_attempts: 2
    enable_parallel: true # Enable parallel agent execution where possible
    max_parallel_agents: 3 # Maximum concurrent agents

  # Checkpointing configuration for workflow resumption
  checkpointing:
    checkpoint_dir: .var/checkpoints # Directory to store workflow checkpoints; set via config or defaults to .var/checkpoints

  quality:
    refinement_threshold: 8.0
    enable_refinement: false # DISABLED: Refinement disabled for Plan #4
    quality_threshold: 7.0 # Lowered from 8.0 to reduce refinement cycles
    # Judge-based refinement settings (DISABLED in Plan #4 optimization)
    judge_threshold: 7.0 # Trigger refinement when Judge score < 7.0 (raised from 5.0)

    max_refinement_rounds: 1 # Set to 1 to satisfy schema validation, but disabled via enable_refinement
    judge_model: gpt-5-mini # Use gpt-5-mini for faster evaluation
    judge_reasoning_effort: minimal # Use low reasoning effort for Judge
    judge_timeout_seconds: 30 # Timeout for judge evaluation (new)
    refinement_min_improvement: 1.0 # Skip refinement if not improving by 1 point

  performance:
    enable_caching: true # Enable response caching
    enable_performance_tracking: true # Track execution metrics
    slow_execution_threshold: 30 # Log executions over 30 seconds
    enable_early_stopping: true # Stop refinement if not improving

  handoffs:
    enabled: true

# Agent Configuration
agents:
  researcher:
    model: gpt-4.1-mini
    tools:
      - TavilySearchTool # Requires TAVILY_API_KEY environment variable
    temperature: 0.5
    enable_dspy: true # Enable DSPy task enhancement
    cache_ttl: 300 # Cache responses for 5 minutes
    timeout: 60 # Task timeout in seconds

  analyst:
    model: gpt-5.1-codex
    tools:
      - HostedCodeInterpreterTool
    temperature: 0.3
    enable_dspy: true
    cache_ttl: 300
    timeout: 90 # Longer timeout for computations
    strategy: "react" # Switch to ReAct for stability

  writer:
    model: gpt-5.1-chat
    tools: []
    temperature: 0.7
    enable_dspy: true
    cache_ttl: 600 # Cache longer for writing
    timeout: 60

  reviewer:
    model: gpt-5-mini # Changed from DeepSeek-R1-0528 (not supported by Responses API)
    tools: []
    temperature: 0.2
    enable_dspy: true
    cache_ttl: 300
    timeout: 45

  # New expanded agent roster (configuration-driven; keeps legacy agents for compatibility)
  planner:
    model: gpt-5-mini # Changed from Kimi-K2-Thinking (not supported by Responses API)
    tools: []
    temperature: 0.5
    instructions: prompts.planner # Uses PlannerInstructionSignature for dynamic generation (DSPy-optimized)
    enable_dspy: true
    cache_ttl: 300
    timeout: 60

  # Other agents (executor, coder, verifier, etc.) currently use static prompts from prompts.py
  # Future: Can be migrated to use AgentInstructionSignature for dynamic generation
  executor:
    model: gpt-5-mini
    tools: []
    temperature: 0.6
    instructions: prompts.executor
    enable_dspy: true
    cache_ttl: 300
    timeout: 60

  coder:
    model: gpt-5-mini
    tools:
      - HostedCodeInterpreterTool
    temperature: 0.3
    instructions: prompts.coder
    enable_dspy: true
    cache_ttl: 300
    timeout: 90

  verifier:
    model: gpt-5-mini
    tools: []
    temperature: 0.5
    instructions: prompts.verifier
    enable_dspy: true
    cache_ttl: 300
    timeout: 60

  generator:
    model: gpt-5-mini
    tools: []
    temperature: 0.8
    instructions: prompts.generator
    enable_dspy: true
    cache_ttl: 600
    timeout: 60

  copilot_researcher:
    model: gpt-5-mini
    tools:
      - PackageSearchMCPTool
      - Context7DeepWikiTool
      - TavilyMCPTool
      - BrowserTool
    temperature: 0.4
    instructions: prompts.copilot_researcher
    enable_dspy: true
    cache_ttl: 300
    timeout: 120

  codex_agent:
    model: gpt-5-mini
    tools:
      - HostedCodeInterpreterTool
    temperature: 0.3
    instructions: prompts.coder
    enable_dspy: true
    cache_ttl: 300
    timeout: 120
    cleanup_files: false # Keep generated files for user download
    description: "Python code execution agent with Code Interpreter for file generation, data analysis, and computations"
    capabilities:
      - file_generation
      - data_analysis

# Microsoft Foundry Hosted Agent - Code Interpreter
# This agent is hosted on Microsoft Foundry with Code Interpreter enabled in the portal
# FinancialAnalyst:
#   type: foundry
#   agent_id: "asst_0123456789abcdef" # Replace with actual Agent ID
#   description: "A specialized financial analyst agent hosted on Azure."
#   tools:
#     - name: "bing_search"
#     - name: "stock_data_api"
#   capabilities:
#     - "market_analysis"
#     - "financial_forecasting"

# Tool Configuration
tools:
  enable_tool_aware_routing: true
  pre_analysis_tool_usage: true
  tool_registry_cache: true
  tool_usage_tracking: true

# Logging
logging:
  level: DEBUG
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  file: .var/logs/workflow.log
  save_history: true
  history_file: .var/logs/execution_history.jsonl
  verbose: true
  # Log verbose GPT-5 reasoning tokens to execution history (default: false)
  # Enable for debugging/evaluation; disable in production to reduce storage
  log_reasoning: true

# UI Routing Configuration
# Maps StreamEventType (and optional kind) to EventCategory and UIHint for frontend rendering
ui_routing:
  # Orchestrator thought events - categorized by kind
  orchestrator_thought:
    routing:
      category: planning
      component: ChatStep
      priority: high
      collapsible: true
      icon_hint: routing
    analysis:
      category: thought
      component: ChatStep
      priority: medium
      collapsible: true
      icon_hint: analysis
    quality:
      category: output
      component: ChatStep
      priority: medium
      collapsible: true
      icon_hint: quality
    progress:
      category: status
      component: ChatStep
      priority: low
      collapsible: true
      icon_hint: progress
    handoff:
      category: planning
      component: ChatStep
      priority: high
      collapsible: false
      icon_hint: handoff
    _default:
      category: thought
      component: ChatStep
      priority: medium
      collapsible: true

  # Orchestrator message events
  orchestrator_message:
    _default:
      category: status
      component: ChatStep
      priority: low
      collapsible: true

  # Agent lifecycle events
  agent_start:
    _default:
      category: step
      component: ChatStep
      priority: low
      collapsible: true
      icon_hint: agent_start
  agent_complete:
    _default:
      category: step
      component: ChatStep
      priority: low
      collapsible: true
      icon_hint: agent_complete
  agent_message:
    _default:
      category: output
      component: MessageBubble
      priority: medium
      collapsible: false
  agent_output:
    _default:
      category: output
      component: MessageBubble
      priority: high
      collapsible: false

  # Reasoning events (GPT-5 chain-of-thought)
  reasoning_delta:
    _default:
      category: reasoning
      component: Reasoning
      priority: medium
      collapsible: true
  reasoning_completed:
    _default:
      category: reasoning
      component: Reasoning
      priority: medium
      collapsible: true

  # Response events
  response_delta:
    _default:
      category: response
      component: MessageBubble
      priority: high
      collapsible: false
  response_completed:
    _default:
      category: response
      component: MessageBubble
      priority: high
      collapsible: false

  # Error events
  error:
    _default:
      category: error
      component: ErrorStep
      priority: high
      collapsible: false

  # Status events
  cancelled:
    _default:
      category: status
      component: ChatStep
      priority: medium
      collapsible: false
      icon_hint: cancelled
  connected:
    _default:
      category: status
      component: ChatStep
      priority: low
      collapsible: false
      icon_hint: connected
  heartbeat:
    _default:
      category: status
      component: ChatStep
      priority: low
      collapsible: true
      icon_hint: heartbeat
  done:
    _default:
      category: status
      component: ChatStep
      priority: low
      collapsible: true

  # Fallback for unknown event types
  _fallback:
    category: status
    component: ChatStep
    priority: low
    collapsible: true

# Tracing / Observability
tracing:
  enabled: true # Set to false to disable all tracing
  otlp_endpoint: ${OTLP_ENDPOINT:-https://cloud.langfuse.com/api/public/otel} # Langfuse OTLP endpoint; configurable via OTLP_ENDPOINT env var for region-specific endpoints (us.cloud.langfuse.com, cloud.langfuse.com, etc.)
  capture_sensitive: false # Capture prompts & completions (set to true only for debugging - GDPR/privacy risk). Default: false for production.
  # Azure Monitor / AI Foundry export (optional)
  # Set connection string to export traces to Microsoft AI Foundry
  # Get this from: Foundry Portal > Your Project > Tracing > Manage data source > Connection string
  # Or set APPLICATIONINSIGHTS_CONNECTION_STRING env var
  azure_monitor_connection_string: # e.g., "InstrumentationKey=xxx;IngestionEndpoint=https://xxx.applicationinsights.azure.com/"
  # NOTE: Data Residency & Compliance
  # If using Langfuse for tracing sensitive workloads, ensure:
  # 1. OTLP_ENDPOINT is set to a region-specific endpoint (us.cloud.langfuse.com for US, cloud.langfuse.com for EU)
  # 2. A Data Processing Agreement (DPA) is in place with Langfuse
  # 3. For highly sensitive data, use a Langfuse plan with data-masking or disable capture_sensitive

# Evaluation Framework
evaluation:
  enabled: true # Enable to run batch evaluations (Option B activated)
  dataset_path: src/agentic_fleet/data/evaluation_tasks.jsonl
  output_dir: .var/logs/evaluation
  metrics:
    - quality_score
    - keyword_success
    - latency_seconds
    - routing_efficiency
    - refinement_triggered
    - relevance_score # Fraction of keywords present (semantic proxy)
    - token_count # Approximate output token count
    - estimated_cost_usd # Estimated model cost based on token_count
  max_tasks: 5 # 0 = no limit
  stop_on_failure: true # Abort early on first failed success metric
  # Phase 4: Disable caching during evaluation for accurate measurements
  disable_caching: false # Set to true to disable all caching during evaluation runs
