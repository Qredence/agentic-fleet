# âœ… Tracing Setup Complete

## What's Been Set Up

You now have a **fully configured agent visualization system** using OpenTelemetry tracing with an OTLP endpoint at `http://localhost:4319`.

---

## ğŸ“¦ What You Got

### 1. **Configuration** âœ“

- **File**: `.env`
- **Updates**:
  ```env
  ENABLE_OTEL=true
  OTLP_ENDPOINT=http://localhost:4319
  ENABLE_SENSITIVE_DATA=true
  ```

### 2. **Infrastructure** âœ“

- **Docker Compose**: `docker/docker-compose.tracing.yml`
  - Runs Jaeger with OTLP collector
  - OTLP/gRPC endpoint: port 4319
  - Jaeger UI: http://localhost:16686

### 3. **Helper Scripts** âœ“

- `scripts/start_tracing.sh` â†’ Start collector
- `scripts/stop_tracing.sh` â†’ Stop collector
- **Also available as**: `make tracing-start` / `make tracing-stop`

### 4. **Documentation** âœ“

- **Quick Reference**: `docs/guides/TRACING_QUICK_REF.md` (1 page)
- **Setup Guide**: `docs/guides/tracing-visualization-setup.md` (detailed)
- **Summary**: `docs/guides/TRACING_SETUP.md`
- **Index**: `docs/guides/INDEX_TRACING.md` (navigation)
- **Complete**: `docs/guides/tracing.md` (reference)

### 5. **Makefile Targets** âœ“

```makefile
make tracing-start    # Launch Jaeger + collector
make tracing-stop     # Stop the collector
```

---

## ğŸš€ Quick Start (Copy & Paste)

```bash
# Terminal 1: Start the tracing collector
make tracing-start

# Terminal 2: Start AgenticFleet backend
make backend

# Terminal 3: Run a workflow and see it trace!
agentic-fleet run -m "Who is the CEO of Apple?" --verbose

# Browser: View traces
# Open http://localhost:16686
# Select service: agentic-fleet
# Click "Find Traces" to see your workflow!
```

---

## ğŸ“Š What You Can See

Once you run a workflow, open **http://localhost:16686** and you'll see:

1. **Service**: `agentic-fleet`
2. **Trace Timeline**:
   - How long your entire workflow took
   - Which phases ran (analysis â†’ routing â†’ execution â†’ quality)
   - Breakdown of time spent in each phase

3. **Span Details**:
   - Agent names and execution duration
   - Tool invocations and results
   - Model API latencies
   - Error details if something fails

Example trace:

```
handle_workflow (2.5s total)
â”œâ”€â”€ analysis_phase (0.8s)
â”œâ”€â”€ routing_phase (0.4s)
â”œâ”€â”€ execution_phase (1.0s)
â”‚  â”œâ”€â”€ agent_1 (researcher)
â”‚  â””â”€â”€ agent_2 (analyzer)
â”œâ”€â”€ quality_phase (0.2s)
â””â”€â”€ final_response
```

---

## âš™ï¸ How It Works

```
Your Code
    â†“
Agent Framework (auto-instruments with OpenTelemetry)
    â†“
OTLP/gRPC protocol
    â†“ http://localhost:4319
Jaeger Collector (running in Docker)
    â†“
Jaeger UI (http://localhost:16686)
    â† You view traces here!
```

---

## ğŸ“š Documentation Guide

| Document                           | Read When                                       | Time   |
| ---------------------------------- | ----------------------------------------------- | ------ |
| **TRACING_QUICK_REF.md**           | You want quick commands                         | 2 min  |
| **TRACING_SETUP.md**               | You want to understand what was set up          | 5 min  |
| **tracing-visualization-setup.md** | You want detailed setup + Jaeger tutorial       | 10 min |
| **tracing.md**                     | You need complete reference (cloud setup, etc.) | 20 min |
| **INDEX_TRACING.md**               | You want to navigate all docs                   | 1 min  |

---

## ğŸ” Security Note

**`ENABLE_SENSITIVE_DATA=true` is set** in your `.env`, which means:

- âœ… Your local backend **captures prompts and model outputs** in traces
- âœ… Perfect for **debugging** locally
- âŒ **Turn OFF in production** with `ENABLE_SENSITIVE_DATA=false`

For production/cloud setups, see `docs/guides/tracing.md` â†’ "Sensitive Data Handling"

---

## ğŸ”— Important Ports

| Port  | Service              | Use                       |
| ----- | -------------------- | ------------------------- |
| 4319  | OTLP/gRPC Endpoint   | Backend sends traces here |
| 16686 | Jaeger UI            | View traces in browser    |
| 8000  | AgenticFleet Backend | Your API server           |
| 5173  | Frontend             | Web UI (if running)       |

**Note**: Port 16686 is the Jaeger UI (for viewing). Port 4319 is where the backend **sends** traces (gRPC protocol). Don't mix them up!

---

## âœ¨ Now You Can:

âœ… See **every agent** that ran and how long it took
âœ… Identify **bottlenecks** in your workflow (which phase is slow?)
âœ… **Debug tool failures** with full error details
âœ… Compare **multiple runs** side-by-side
âœ… Export **trace data** for analysis
âœ… Understand **multi-agent interactions** visually

---

## ğŸ“ Next Steps

1. **Run `make tracing-start`** to launch Jaeger
2. **Run `make backend`** to start AgenticFleet
3. **Execute a workflow**: `agentic-fleet run -m "your task"`
4. **Open http://localhost:16686** and explore!

---

## ğŸ†˜ Quick Troubleshooting

| Problem              | Fix                                           |
| -------------------- | --------------------------------------------- |
| "Connection refused" | Run `make tracing-start` first                |
| No traces in Jaeger  | Verify `ENABLE_OTEL=true` in .env             |
| Can't see prompts    | `ENABLE_SENSITIVE_DATA=true` is already set   |
| Jaeger UI is empty   | Make sure backend is running (`make backend`) |

For more help, see **TRACING_QUICK_REF.md** or **tracing-visualization-setup.md**.

---

## ğŸ¯ Files Summary

```
âœ“ .env                           (updated with OTLP endpoint)
âœ“ docker/docker-compose.tracing.yml     (Docker setup for Jaeger)
âœ“ scripts/start_tracing.sh       (helper script)
âœ“ scripts/stop_tracing.sh        (helper script)
âœ“ Makefile                       (added tracing-start/stop targets)
âœ“ docs/guides/TRACING_SETUP.md                    (this summary)
âœ“ docs/guides/TRACING_QUICK_REF.md               (quick reference)
âœ“ docs/guides/tracing-visualization-setup.md     (detailed guide)
âœ“ docs/guides/INDEX_TRACING.md                   (navigation)
```

---

## ğŸš€ You're Ready!

Everything is configured and documented. Start tracing your agents now:

```bash
make tracing-start && make backend
# Then open http://localhost:16686
```

Happy debugging! ğŸ‰
